"""
Semantic Code Indexing Pipeline

This module provides a comprehensive semantic analysis and indexing system for Python codebases.
It extracts semantic information from source code using AST analysis, generates embeddings,
and builds a knowledge graph for efficient code search and analysis.

The pipeline consists of four main phases:
1. AST Analysis: Extract semantic entities (functions, classes, methods) and relationships
2. Embedding Generation: Create vector embeddings using multiple approaches (text, structure, context)
3. Knowledge Graph Construction: Build a graph database with entities and relationships
4. Search & Query: Enable semantic and graph-based search over the indexed codebase

Key Features:
- Multi-threaded AST analysis for performance
- Hybrid embedding generation (text + structure + context)
- SQLite-based knowledge graph storage
- Semantic similarity search using vector embeddings
- Graph traversal for relationship discovery
- Comprehensive CLI interface with multiple operation modes

Dependencies:
- ast: Python AST parsing
- sqlite3: Knowledge graph storage
- numpy: Vector operations and embeddings
- concurrent.futures: Parallel processing
- pathlib: File system operations

Author: Generated by AI Assistant
License: MIT
"""

# ===== COMPLETE SEMANTIC CODE INDEXING PIPELINE =====
# AST Analysis → Semantic Indexing → Embeddings → Knowledge Graph/Vector Store

import ast
import json
import hashlib
import logging
import traceback
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import numpy as np
from concurrent.futures import ProcessPoolExecutor
import sqlite3
import yaml
import re
from datetime import datetime
from abc import ABC, abstractmethod
import argparse
import sys
import time

# ===== 1. CONTEXTUAL KNOWLEDGE SYSTEM =====

@dataclass
class BusinessDomain:
    """
    Represents a business domain with associated patterns and importance.
    
    Attributes:
        name (str): Domain name (e.g., "Authentication", "Payment")
        keywords (List[str]): Keywords associated with this domain
        patterns (List[str]): Regex patterns to match domain-related code
        importance (float): Importance multiplier for this domain
    """
    name: str
    keywords: List[str]
    patterns: List[str]
    importance: float

@dataclass
class ArchitecturalPattern:
    """
    Represents an architectural pattern with quality metrics.
    
    Attributes:
        name (str): Pattern name (e.g., "MVC", "Repository")
        indicators (List[str]): Keywords that indicate this pattern
        quality_metrics (Dict[str, Any]): Quality requirements for this pattern
        relationships (List[str]): Expected relationships in this pattern
    """
    name: str
    indicators: List[str]
    quality_metrics: Dict[str, Any]
    relationships: List[str]

@dataclass
class TeamOwnership:
    """
    Represents team ownership and preferences.
    
    Attributes:
        name (str): Team name
        owned_modules (List[str]): Module paths owned by this team
        expertise (List[str]): Areas of expertise
        contact (str): Contact information
        code_style (Dict[str, Any]): Code style preferences
    """
    name: str
    owned_modules: List[str]
    expertise: List[str]
    contact: str
    code_style: Dict[str, Any]

@dataclass
class ContextualAnnotation:
    """
    Represents contextual annotations for specific code entities.
    
    Attributes:
        file_path (str): Path to the file
        function_name (str): Name of the function
        category (str): Annotation category (e.g., "performance", "security")
        annotations (Dict[str, Any]): Specific annotations
    """
    file_path: str
    function_name: str
    category: str
    annotations: Dict[str, Any]

class ContextualKnowledgeLoader:
    """
    Loads and manages contextual knowledge from configuration files.
    
    This class reads YAML configuration files that define:
    - Business domains and their importance
    - Architectural patterns and quality metrics
    - Team ownership and expertise
    - Specific annotations for code entities
    """
    
    def __init__(self, config_dir: str = "./context_config"):
        """
        Initialize the contextual knowledge loader.
        
        Args:
            config_dir (str): Directory containing configuration YAML files
        """
        self.config_dir = Path(config_dir)
        self.business_domains: List[BusinessDomain] = []
        self.architectural_patterns: List[ArchitecturalPattern] = []
        self.teams: List[TeamOwnership] = []
        self.annotations: Dict[str, List[ContextualAnnotation]] = {}
        
        if self.config_dir.exists():
            self._load_configurations()
        else:
            print(f"⚠️  Context config directory '{config_dir}' not found. Using default configurations.")
            self._load_default_configurations()
    
    def _load_configurations(self):
        """Load all configuration files from the config directory."""
        # Load business domains
        domains_file = self.config_dir / "business_domains.yaml"
        if domains_file.exists():
            self._load_business_domains(domains_file)
        
        # Load architectural patterns
        patterns_file = self.config_dir / "architectural_patterns.yaml"
        if patterns_file.exists():
            self._load_architectural_patterns(patterns_file)
        
        # Load team information
        teams_file = self.config_dir / "teams.yaml"
        if teams_file.exists():
            self._load_teams(teams_file)
        
        # Load annotations
        annotations_file = self.config_dir / "annotations.yaml"
        if annotations_file.exists():
            self._load_annotations(annotations_file)
    
    def _load_business_domains(self, file_path: Path):
        """Load business domain configurations."""
        with open(file_path, 'r') as f:
            data = yaml.safe_load(f)
        
        for domain_data in data.get('domains', []):
            domain = BusinessDomain(
                name=domain_data['name'],
                keywords=domain_data.get('keywords', []),
                patterns=domain_data.get('patterns', []),
                importance=domain_data.get('importance', 1.0)
            )
            self.business_domains.append(domain)
    
    def _load_architectural_patterns(self, file_path: Path):
        """Load architectural pattern configurations."""
        with open(file_path, 'r') as f:
            data = yaml.safe_load(f)
        
        for pattern_data in data.get('patterns', []):
            pattern = ArchitecturalPattern(
                name=pattern_data['name'],
                indicators=pattern_data.get('indicators', []),
                quality_metrics=pattern_data.get('quality_metrics', {}),
                relationships=pattern_data.get('relationships', [])
            )
            self.architectural_patterns.append(pattern)
    
    def _load_teams(self, file_path: Path):
        """Load team ownership configurations."""
        with open(file_path, 'r') as f:
            data = yaml.safe_load(f)
        
        for team_data in data.get('teams', []):
            team = TeamOwnership(
                name=team_data['name'],
                owned_modules=team_data.get('owned_modules', []),
                expertise=team_data.get('expertise', []),
                contact=team_data.get('contact', ''),
                code_style=team_data.get('code_style', {})
            )
            self.teams.append(team)
    
    def _load_annotations(self, file_path: Path):
        """Load contextual annotations."""
        with open(file_path, 'r') as f:
            data = yaml.safe_load(f)
        
        for category, category_data in data.items():
            for file_func, annotations in category_data.items():
                parts = file_func.split(':')
                if len(parts) >= 2:
                    file_path = parts[0]
                    function_name = parts[1]
                    
                    annotation = ContextualAnnotation(
                        file_path=file_path,
                        function_name=function_name,
                        category=category,
                        annotations=annotations
                    )
                    
                    key = f"{file_path}:{function_name}"
                    if key not in self.annotations:
                        self.annotations[key] = []
                    self.annotations[key].append(annotation)
    
    def _load_default_configurations(self):
        """
        Load default contextual configurations when config files are not available.
        
        This provides sensible defaults for common software development patterns,
        business domains, and architectural patterns.
        """
        
        # Default business domains
        self.business_domains = [
            BusinessDomain(
                name="Authentication",
                keywords=["auth", "login", "password", "token", "jwt", "oauth", "user", "session", "security"],
                patterns=[r".*auth.*", r".*login.*", r".*user.*", r".*session.*"],
                importance=1.8
            ),
            BusinessDomain(
                name="Database",
                keywords=["sql", "query", "database", "db", "table", "migration", "orm", "model", "repository"],
                patterns=[r".*db.*", r".*sql.*", r".*model.*", r".*repo.*"],
                importance=1.4
            ),
            BusinessDomain(
                name="API",
                keywords=["api", "endpoint", "rest", "http", "request", "response", "route", "handler"],
                patterns=[r".*api.*", r".*endpoint.*", r".*route.*", r".*handler.*"],
                importance=1.5
            ),
            BusinessDomain(
                name="Payment",
                keywords=["payment", "billing", "invoice", "subscription", "charge", "refund", "transaction"],
                patterns=[r".*payment.*", r".*billing.*", r".*invoice.*", r".*transaction.*"],
                importance=2.0
            ),
            BusinessDomain(
                name="Core",
                keywords=["core", "util", "common", "shared", "base", "foundation"],
                patterns=[r".*core.*", r".*util.*", r".*common.*", r".*shared.*"],
                importance=1.6
            ),
            BusinessDomain(
                name="UI",
                keywords=["ui", "view", "template", "render", "display", "frontend", "component"],
                patterns=[r".*ui.*", r".*view.*", r".*template.*", r".*component.*"],
                importance=1.2
            ),
            BusinessDomain(
                name="Configuration",
                keywords=["config", "settings", "env", "environment", "setup", "init"],
                patterns=[r".*config.*", r".*settings.*", r".*env.*", r".*setup.*"],
                importance=1.3
            ),
            BusinessDomain(
                name="Testing",
                keywords=["test", "mock", "fixture", "assert", "spec", "unit", "integration"],
                patterns=[r".*test.*", r".*mock.*", r".*spec.*"],
                importance=1.1
            )
        ]
        
        # Default architectural patterns
        self.architectural_patterns = [
            ArchitecturalPattern(
                name="MVC",
                indicators=["controller", "view", "model"],
                quality_metrics={
                    "max_complexity": 10,
                    "min_test_coverage": 0.8,
                    "single_responsibility": True
                },
                relationships=["controller_uses_model", "controller_renders_view"]
            ),
            ArchitecturalPattern(
                name="Repository",
                indicators=["repository", "repo"],
                quality_metrics={
                    "interface_required": True,
                    "max_complexity": 8,
                    "single_responsibility": True
                },
                relationships=["service_uses_repository"]
            ),
            ArchitecturalPattern(
                name="Service",
                indicators=["service", "manager"],
                quality_metrics={
                    "max_complexity": 12,
                    "interface_required": False
                },
                relationships=["service_uses_repository", "controller_uses_service"]
            ),
            ArchitecturalPattern(
                name="Factory",
                indicators=["factory", "builder", "create"],
                quality_metrics={
                    "single_responsibility": True,
                    "max_complexity": 6
                },
                relationships=["creates_objects"]
            ),
            ArchitecturalPattern(
                name="Handler",
                indicators=["handler", "processor", "executor"],
                quality_metrics={
                    "max_complexity": 15,
                    "error_handling_required": True
                },
                relationships=["handles_requests"]
            ),
            ArchitecturalPattern(
                name="Validator",
                indicators=["validator", "validate", "check"],
                quality_metrics={
                    "max_complexity": 8,
                    "comprehensive_coverage": True
                },
                relationships=["validates_input"]
            )
        ]
        
        # Default teams (generic development team structure)
        self.teams = [
            TeamOwnership(
                name="Backend Team",
                owned_modules=["api/", "core/", "services/", "models/"],
                expertise=["api_development", "database_design", "performance", "scalability"],
                contact="backend-team@company.com",
                code_style={
                    "max_line_length": 100,
                    "mandatory_tests": True,
                    "prefer_composition": True,
                    "error_handling_required": True
                }
            ),
            TeamOwnership(
                name="Frontend Team", 
                owned_modules=["ui/", "views/", "templates/", "static/"],
                expertise=["user_interface", "user_experience", "frontend_frameworks"],
                contact="frontend-team@company.com",
                code_style={
                    "max_line_length": 120,
                    "component_based": True,
                    "accessibility_required": True
                }
            ),
            TeamOwnership(
                name="Security Team",
                owned_modules=["auth/", "security/", "crypto/"],
                expertise=["security", "authentication", "authorization", "cryptography"],
                contact="security-team@company.com",
                code_style={
                    "mandatory_tests": True,
                    "security_first": True,
                    "audit_logging": True,
                    "input_validation": True
                }
            ),
            TeamOwnership(
                name="Data Team",
                owned_modules=["data/", "analytics/", "ml/", "etl/"],
                expertise=["data_analysis", "machine_learning", "data_engineering"],
                contact="data-team@company.com",
                code_style={
                    "documentation_required": True,
                    "reproducible_results": True,
                    "data_validation": True
                }
            ),
            TeamOwnership(
                name="Infrastructure Team",
                owned_modules=["deploy/", "config/", "scripts/", "ops/"],
                expertise=["devops", "infrastructure", "deployment", "monitoring"],
                contact="infra-team@company.com",
                code_style={
                    "idempotent_scripts": True,
                    "monitoring_required": True,
                    "rollback_capability": True
                }
            )
        ]
        
        # Default annotations (empty - these are typically project-specific)
        self.annotations = {}
        
        print(f"✅ Loaded default configurations:")
        print(f"   📊 {len(self.business_domains)} business domains")
        print(f"   🏗️  {len(self.architectural_patterns)} architectural patterns") 
        print(f"   👥 {len(self.teams)} team structures")
    
    def create_default_config_files(self, output_dir: str = "./context_config"):
        """
        Create default configuration files that users can customize.
        
        Args:
            output_dir (str): Directory where config files will be created
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        print(f"📁 Creating default configuration files in {output_dir}")
        
        # Create default business domains config
        domains_config = {
            'domains': [
                {
                    'name': domain.name,
                    'keywords': domain.keywords,
                    'patterns': domain.patterns,
                    'importance': domain.importance
                }
                for domain in self.business_domains
            ]
        }
        
        with open(output_path / "business_domains.yaml", 'w') as f:
            yaml.safe_dump(domains_config, f, default_flow_style=False, indent=2)
        
        # Create default architectural patterns config
        patterns_config = {
            'patterns': [
                {
                    'name': pattern.name,
                    'indicators': pattern.indicators,
                    'quality_metrics': pattern.quality_metrics,
                    'relationships': pattern.relationships
                }
                for pattern in self.architectural_patterns
            ]
        }
        
        with open(output_path / "architectural_patterns.yaml", 'w') as f:
            yaml.safe_dump(patterns_config, f, default_flow_style=False, indent=2)
        
        # Create default teams config
        teams_config = {
            'teams': [
                {
                    'name': team.name,
                    'owned_modules': team.owned_modules,
                    'expertise': team.expertise,
                    'contact': team.contact,
                    'code_style': team.code_style
                }
                for team in self.teams
            ]
        }
        
        with open(output_path / "teams.yaml", 'w') as f:
            yaml.safe_dump(teams_config, f, default_flow_style=False, indent=2)
        
        # Create empty annotations config with examples
        annotations_config = {
            'performance': {
                'example/api.py:process_request': {
                    'critical_path': True,
                    'expected_latency_ms': 200,
                    'rate_limit': '100/minute'
                }
            },
            'security': {
                'example/auth.py:validate_token': {
                    'audit_required': True,
                    'security_level': 'critical',
                    'encryption_required': True
                }
            }
        }
        
        with open(output_path / "annotations.yaml", 'w') as f:
            yaml.safe_dump(annotations_config, f, default_flow_style=False, indent=2)
        
        print(f"✅ Created configuration files:")
        print(f"   📊 business_domains.yaml - {len(self.business_domains)} domains")
        print(f"   🏗️  architectural_patterns.yaml - {len(self.architectural_patterns)} patterns")
        print(f"   👥 teams.yaml - {len(self.teams)} teams")
        print(f"   📝 annotations.yaml - example annotations")
        print(f"\n💡 Customize these files for your specific project needs!")
    
    def get_business_domain(self, entity_name: str, file_path: str) -> Optional[BusinessDomain]:
        """Identify the business domain for a code entity."""
        entity_text = f"{entity_name} {file_path}".lower()
        
        best_match = None
        best_score = 0
        
        for domain in self.business_domains:
            score = 0
            
            # Check keyword matches
            for keyword in domain.keywords:
                if keyword.lower() in entity_text:
                    score += 1
            
            # Check pattern matches
            for pattern in domain.patterns:
                if re.search(pattern, entity_text, re.IGNORECASE):
                    score += 2  # Patterns have higher weight
            
            if score > best_score:
                best_score = score
                best_match = domain
        
        return best_match
    
    def get_architectural_pattern(self, entity_name: str, entity_type: str) -> Optional[ArchitecturalPattern]:
        """Identify architectural patterns for a code entity."""
        entity_text = f"{entity_name} {entity_type}".lower()
        
        for pattern in self.architectural_patterns:
            for indicator in pattern.indicators:
                if indicator.lower() in entity_text:
                    return pattern
        
        return None
    
    def get_team_ownership(self, file_path: str) -> Optional[TeamOwnership]:
        """Identify team ownership for a file."""
        for team in self.teams:
            for module in team.owned_modules:
                if file_path.startswith(module):
                    return team
        
        return None
    
    def get_annotations(self, file_path: str, function_name: str) -> List[ContextualAnnotation]:
        """Get annotations for a specific code entity."""
        key = f"{file_path}:{function_name}"
        return self.annotations.get(key, [])

# ===== 2. ENHANCED AST EXTRACTION WITH CONTEXTUAL KNOWLEDGE =====

@dataclass
class CodeEntity:
    """
    Represents a semantic code entity extracted from source code.
    
    This class encapsulates all relevant information about a code entity
    (function, class, method, variable) including its semantic context,
    structural properties, and relationships.
    
    Attributes:
        id (str): Unique identifier for the entity (format: file:name:line)
        type (str): Entity type ('function', 'class', 'method', 'variable')
        name (str): Simple name of the entity
        full_name (str): Fully qualified name including module/class path
        source_code (str): Raw source code of the entity
        docstring (Optional[str]): Docstring if present
        file_path (str): Path to the source file
        line_start (int): Starting line number (1-indexed)
        line_end (int): Ending line number (1-indexed)
        complexity (int): Cyclomatic complexity score
        dependencies (List[str]): List of entities this one depends on
        usages (List[str]): List of places where this entity is used
        context (Dict[str, Any]): Additional semantic context and metadata
    """
    id: str
    type: str  # 'function', 'class', 'method', 'variable'
    name: str
    full_name: str  # Including module path
    source_code: str
    docstring: Optional[str]
    file_path: str
    line_start: int
    line_end: int
    complexity: int
    dependencies: List[str]  # What this entity depends on
    usages: List[str]  # Where this entity is used
    context: Dict[str, Any]  # Additional semantic context
    business_domain: Optional[str] = None  # Business domain classification
    architectural_pattern: Optional[str] = None  # Architectural pattern
    team_ownership: Optional[str] = None  # Owning team
    contextual_annotations: List[Dict[str, Any]] = None  # Contextual annotations
    importance_score: float = 1.0  # Calculated importance based on context

@dataclass
class CodeRelation:
    """
    Represents relationships between code entities.
    
    This class captures various types of relationships that exist between
    code entities, such as function calls, inheritance, imports, etc.
    
    Attributes:
        source_id (str): ID of the source entity
        target_id (str): ID of the target entity
        relation_type (str): Type of relationship ('calls', 'inherits', 'imports', 'uses', 'contains')
        confidence (float): Confidence score for the relationship (0.0 to 1.0)
        context (Dict[str, Any]): Additional context about the relationship
    """
    source_id: str
    target_id: str
    relation_type: str  # 'calls', 'inherits', 'imports', 'uses', 'contains'
    confidence: float
    context: Dict[str, Any]

class SemanticASTAnalyzer:
    """
    Enhanced AST analyzer that extracts semantic information suitable for embedding generation and knowledge graphs.
    
    This class performs comprehensive analysis of Python codebases by:
    1. Parsing AST trees from Python files
    2. Extracting semantic entities (functions, classes, methods)
    3. Building relationship graphs between entities
    4. Computing complexity metrics and semantic context
    5. Supporting parallel processing for large codebases
    
    The analyzer uses a multi-phase approach:
    - Phase 1: Extract entities from individual files in parallel
    - Phase 2: Build cross-file relationships
    - Phase 3: Enhance with semantic context and patterns
    
    Attributes:
        entities (List[CodeEntity]): Collected code entities from analysis
        relations (List[CodeRelation]): Discovered relationships between entities
        call_graph (Dict[str, List[str]]): Function call graph
        inheritance_tree (Dict[str, List[str]]): Class inheritance relationships
    """
    
    def __init__(self, context_config_dir: str = "./context_config"):
        """
        Initialize the analyzer with empty collections and contextual knowledge.
        
        Args:
            context_config_dir (str): Directory containing contextual configuration files
        """
        self.entities: List[CodeEntity] = []
        self.relations: List[CodeRelation] = []
        self.call_graph: Dict[str, List[str]] = {}
        self.inheritance_tree: Dict[str, List[str]] = {}
        self.context_loader = ContextualKnowledgeLoader(context_config_dir)
        
    def analyze_codebase(self, root_path: str) -> Tuple[List[CodeEntity], List[CodeRelation]]:
        """
        Perform comprehensive semantic analysis of a codebase.
        
        This method orchestrates the complete analysis pipeline:
        1. Discovers all Python files in the codebase
        2. Analyzes each file in parallel using ProcessPoolExecutor
        3. Builds cross-file relationships and dependencies
        4. Enhances entities with semantic context
        
        Args:
            root_path (str): Root directory path to analyze
            
        Returns:
            Tuple[List[CodeEntity], List[CodeRelation]]: Extracted entities and relationships
            
        Raises:
            FileNotFoundError: If root_path doesn't exist
            PermissionError: If files cannot be read
        """
        
        # Phase 1: Extract entities from each file
        python_files = list(Path(root_path).glob("**/*.py"))
        
        with ProcessPoolExecutor() as executor:
            file_results = list(executor.map(self._analyze_file, python_files))
        
        # Combine results
        for entities, relations in file_results:
            self.entities.extend(entities)
            self.relations.extend(relations)
        
        # Phase 2: Build cross-file relationships
        self._build_cross_file_relationships()
        
        # Phase 3: Enhance with semantic context
        self._enhance_semantic_context()
        
        # Phase 4: Apply contextual knowledge
        self._apply_contextual_knowledge()
        
        return self.entities, self.relations
    
    def _analyze_file(self, file_path: Path) -> Tuple[List[CodeEntity], List[CodeRelation]]:
        """
        Analyze a single Python file for entities and relationships.
        
        This method parses a single Python file and extracts all semantic
        entities and their local relationships. It's designed to be called
        in parallel for performance.
        
        Args:
            file_path (Path): Path to the Python file to analyze
            
        Returns:
            Tuple[List[CodeEntity], List[CodeRelation]]: Entities and relations found in the file
            
        Note:
            Returns empty lists if the file cannot be parsed (syntax errors, encoding issues, etc.)
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                source = f.read()
            
            tree = ast.parse(source, filename=str(file_path))
            
            visitor = SemanticVisitor(str(file_path), source.split('\n'))
            visitor.visit(tree)
            
            return visitor.entities, visitor.relations
            
        except Exception as e:
            # Log error in production - for now, silently skip problematic files
            return [], []
    
    def _build_cross_file_relationships(self):
        """
        Build relationships across files (imports, inheritance, etc.).
        
        This method analyzes dependencies between entities across different files
        to build a complete relationship graph. It looks for:
        - Import relationships
        - Cross-file function calls
        - Inheritance across modules
        - Usage dependencies
        """
        
        # Build entity lookup
        entity_lookup = {entity.full_name: entity for entity in self.entities}
        
        # Find cross-file relationships
        for entity in self.entities:
            for dep in entity.dependencies:
                if dep in entity_lookup:
                    self.relations.append(CodeRelation(
                        source_id=entity.id,
                        target_id=entity_lookup[dep].id,
                        relation_type='depends_on',
                        confidence=0.9,
                        context={'type': 'cross_file_dependency'}
                    ))
    
    def _enhance_semantic_context(self):
        """
        Add semantic context to entities based on patterns and usage.
        
        This method analyzes naming patterns, complexity metrics, and usage
        patterns to add semantic context that helps with categorization
        and search. It identifies:
        - Function categories (test, private, data access, etc.)
        - Complexity levels (low, medium, high)
        - Intent patterns (getters, setters, processors, etc.)
        - Visibility modifiers (public, private, protected)
        """
        
        for entity in self.entities:
            # Analyze naming patterns for semantic intent
            if entity.type == 'function':
                if entity.name.startswith('test_'):
                    entity.context['category'] = 'test'
                elif entity.name.startswith('_'):
                    entity.context['visibility'] = 'private'
                elif any(word in entity.name.lower() for word in ['get', 'fetch', 'retrieve']):
                    entity.context['intent'] = 'data_access'
                elif any(word in entity.name.lower() for word in ['set', 'update', 'modify']):
                    entity.context['intent'] = 'data_modification'
            
            # Classify complexity levels for better categorization
            if entity.complexity > 10:
                entity.context['complexity_level'] = 'high'
            elif entity.complexity > 5:
                entity.context['complexity_level'] = 'medium'
            else:
                entity.context['complexity_level'] = 'low'
    
    def _apply_contextual_knowledge(self):
        """
        Apply contextual knowledge to entities using configuration data.
        
        This method enriches entities with:
        - Business domain classification
        - Architectural pattern identification
        - Team ownership information
        - Contextual annotations
        - Importance scoring based on domain and annotations
        """
        
        for entity in self.entities:
            # Identify business domain
            domain = self.context_loader.get_business_domain(entity.name, entity.file_path)
            if domain:
                entity.business_domain = domain.name
                entity.importance_score *= domain.importance
                entity.context['business_domain'] = domain.name
                entity.context['domain_keywords'] = domain.keywords
            
            # Identify architectural pattern
            pattern = self.context_loader.get_architectural_pattern(entity.name, entity.type)
            if pattern:
                entity.architectural_pattern = pattern.name
                entity.context['architectural_pattern'] = pattern.name
                entity.context['quality_metrics'] = pattern.quality_metrics
                
                # Check if entity meets quality metrics
                self._validate_quality_metrics(entity, pattern)
            
            # Identify team ownership
            team = self.context_loader.get_team_ownership(entity.file_path)
            if team:
                entity.team_ownership = team.name
                entity.context['team_ownership'] = team.name
                entity.context['team_contact'] = team.contact
                entity.context['team_expertise'] = team.expertise
                
                # Apply team-specific code style validation
                self._validate_code_style(entity, team)
            
            # Apply contextual annotations
            annotations = self.context_loader.get_annotations(entity.file_path, entity.name)
            if annotations:
                entity.contextual_annotations = []
                for annotation in annotations:
                    entity.contextual_annotations.append({
                        'category': annotation.category,
                        'annotations': annotation.annotations
                    })
                    
                    # Increase importance for critical annotations
                    if annotation.category == 'security' and 'security_level' in annotation.annotations:
                        if annotation.annotations['security_level'] == 'critical':
                            entity.importance_score *= 2.0
                    
                    if annotation.category == 'performance' and 'critical_path' in annotation.annotations:
                        if annotation.annotations['critical_path']:
                            entity.importance_score *= 1.5
                    
                    # Add annotation context
                    entity.context[f'{annotation.category}_annotations'] = annotation.annotations
    
    def _validate_quality_metrics(self, entity: CodeEntity, pattern: ArchitecturalPattern):
        """
        Validate entity against architectural pattern quality metrics.
        
        Args:
            entity (CodeEntity): Entity to validate
            pattern (ArchitecturalPattern): Pattern with quality requirements
        """
        quality_issues = []
        
        # Check complexity limits
        if 'max_complexity' in pattern.quality_metrics:
            max_complexity = pattern.quality_metrics['max_complexity']
            if entity.complexity > max_complexity:
                quality_issues.append(f"Complexity {entity.complexity} exceeds limit {max_complexity}")
        
        # Check if interface is required
        if pattern.quality_metrics.get('interface_required', False):
            if entity.type == 'class' and not self._has_interface_indicators(entity):
                quality_issues.append("Interface required but not found")
        
        # Check single responsibility
        if pattern.quality_metrics.get('single_responsibility', False):
            if entity.complexity > 15:  # High complexity might indicate multiple responsibilities
                quality_issues.append("Possible violation of single responsibility principle")
        
        if quality_issues:
            entity.context['quality_issues'] = quality_issues
    
    def _validate_code_style(self, entity: CodeEntity, team: TeamOwnership):
        """
        Validate entity against team code style preferences.
        
        Args:
            entity (CodeEntity): Entity to validate
            team (TeamOwnership): Team with style preferences
        """
        style_issues = []
        
        # Check line length
        if 'max_line_length' in team.code_style:
            max_length = team.code_style['max_line_length']
            lines = entity.source_code.split('\n')
            long_lines = [i+1 for i, line in enumerate(lines) if len(line) > max_length]
            if long_lines:
                style_issues.append(f"Lines exceed {max_length} characters: {long_lines[:5]}")
        
        # Check for mandatory tests
        if team.code_style.get('mandatory_tests', False):
            if entity.type in ['function', 'method'] and not self._has_corresponding_test(entity):
                style_issues.append("Test required but not found")
        
        # Check security-first approach
        if team.code_style.get('security_first', False):
            if entity.type in ['function', 'method'] and self._handles_sensitive_data(entity):
                if not self._has_security_checks(entity):
                    style_issues.append("Security checks required for sensitive data handling")
        
        if style_issues:
            entity.context['style_issues'] = style_issues
    
    def _has_interface_indicators(self, entity: CodeEntity) -> bool:
        """Check if entity has interface-like characteristics."""
        # Simple heuristic - look for abstract methods or interface naming
        return ('abstract' in entity.source_code.lower() or 
                'interface' in entity.name.lower() or
                entity.name.endswith('Interface'))
    
    def _has_corresponding_test(self, entity: CodeEntity) -> bool:
        """Check if entity has corresponding test (simplified check)."""
        # This would need to be enhanced with actual test discovery
        test_patterns = [f"test_{entity.name}", f"{entity.name}_test", f"Test{entity.name}"]
        return any(pattern in str(self.entities) for pattern in test_patterns)
    
    def _handles_sensitive_data(self, entity: CodeEntity) -> bool:
        """Check if entity handles sensitive data."""
        sensitive_keywords = ['password', 'token', 'key', 'secret', 'auth', 'login', 'payment', 'credit']
        entity_text = f"{entity.name} {entity.source_code}".lower()
        return any(keyword in entity_text for keyword in sensitive_keywords)
    
    def _has_security_checks(self, entity: CodeEntity) -> bool:
        """Check if entity has security validation."""
        security_indicators = ['validate', 'sanitize', 'encrypt', 'hash', 'verify', 'authenticate']
        return any(indicator in entity.source_code.lower() for indicator in security_indicators)

class SemanticVisitor(ast.NodeVisitor):
    """
    Enhanced AST visitor that extracts semantic information from Python AST nodes.
    
    This visitor traverses the AST and extracts detailed information about
    code entities and their relationships. It maintains context during traversal
    to properly handle nested structures like class methods.
    
    Key features:
    - Tracks current class/function context for proper naming
    - Extracts source code ranges for entities
    - Computes complexity metrics
    - Identifies function calls and dependencies
    - Handles decorators and type annotations
    
    Attributes:
        file_path (str): Path to the file being analyzed
        lines (List[str]): Source code lines for extracting ranges
        entities (List[CodeEntity]): Collected entities from this file
        relations (List[CodeRelation]): Discovered relationships
        current_class (Optional[str]): Name of current class being processed
        current_function (Optional[str]): ID of current function being processed
        imports (Dict): Imported modules and their aliases
        call_stack (List): Stack for tracking nested calls
    """
    
    def __init__(self, file_path: str, lines: List[str]):
        """
        Initialize the visitor with file context.
        
        Args:
            file_path (str): Path to the source file
            lines (List[str]): Source code split into lines
        """
        self.file_path = file_path
        self.lines = lines
        self.entities: List[CodeEntity] = []
        self.relations: List[CodeRelation] = []
        self.current_class = None
        self.current_function = None
        self.imports = {}
        self.call_stack = []
        
    def visit_FunctionDef(self, node):
        """
        Visit function definition nodes and extract semantic information.
        
        This method processes both regular functions and class methods,
        extracting comprehensive information including:
        - Source code ranges
        - Complexity metrics
        - Parameter information
        - Decorator analysis
        - Dependency tracking
        
        Args:
            node: AST FunctionDef node to process
        """
        entity_id = f"{self.file_path}:{node.name}:{node.lineno}"
        
        # Extract function source code
        end_line = self._find_function_end(node)
        source_code = '\n'.join(self.lines[node.lineno-1:end_line])
        
        # Calculate complexity
        complexity_visitor = ComplexityVisitor()
        complexity_visitor.visit(node)
        
        # Extract dependencies
        dep_visitor = DependencyVisitor()
        dep_visitor.visit(node)
        
        entity = CodeEntity(
            id=entity_id,
            type='method' if self.current_class else 'function',
            name=node.name,
            full_name=f"{self.current_class}.{node.name}" if self.current_class else node.name,
            source_code=source_code,
            docstring=ast.get_docstring(node),
            file_path=self.file_path,
            line_start=node.lineno,
            line_end=end_line,
            complexity=complexity_visitor.complexity,
            dependencies=dep_visitor.dependencies,
            usages=[],
            context={
                'args': [arg.arg for arg in node.args.args],
                'decorators': [self._get_name(d) for d in node.decorator_list],
                'returns': self._has_return_statement(node)
            },
            contextual_annotations=[]
        )
        
        self.entities.append(entity)
        
        # Track function calls within this function
        old_function = self.current_function
        self.current_function = entity_id
        self.generic_visit(node)
        self.current_function = old_function
    
    def visit_ClassDef(self, node):
        """
        Visit class definition nodes and extract class information.
        
        This method processes class definitions, extracting information about:
        - Class inheritance hierarchy
        - Class decorators
        - Source code ranges
        - Docstring content
        
        Args:
            node: AST ClassDef node to process
        """
        entity_id = f"{self.file_path}:{node.name}:{node.lineno}"
        
        end_line = self._find_class_end(node)
        source_code = '\n'.join(self.lines[node.lineno-1:end_line])
        
        entity = CodeEntity(
            id=entity_id,
            type='class',
            name=node.name,
            full_name=node.name,
            source_code=source_code,
            docstring=ast.get_docstring(node),
            file_path=self.file_path,
            line_start=node.lineno,
            line_end=end_line,
            complexity=0,
            dependencies=[self._get_name(base) for base in node.bases],
            usages=[],
            context={
                'bases': [self._get_name(base) for base in node.bases],
                'decorators': [self._get_name(d) for d in node.decorator_list]
            },
            contextual_annotations=[]
        )
        
        self.entities.append(entity)
        
        # Process class methods
        old_class = self.current_class
        self.current_class = node.name
        self.generic_visit(node)
        self.current_class = old_class
    
    def visit_Call(self, node):
        """
        Track function calls for building call graph.
        
        This method identifies function calls within the current context
        and creates relationships between calling and called functions.
        This is essential for building the call graph and understanding
        code dependencies.
        
        Args:
            node: AST Call node representing a function call
        """
        if self.current_function:
            func_name = self._get_name(node.func)
            if func_name:
                self.relations.append(CodeRelation(
                    source_id=self.current_function,
                    target_id=f"call:{func_name}",
                    relation_type='calls',
                    confidence=0.8,
                    context={'line': node.lineno}
                ))
        
        self.generic_visit(node)
    
    def _get_name(self, node):
        """
        Extract name from AST node handling different node types.
        
        This utility method handles various AST node types that represent
        names (simple names, attribute access, etc.) and returns a string
        representation.
        
        Args:
            node: AST node that represents a name
            
        Returns:
            str or None: String representation of the name, or None if not extractable
        """
        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            return f"{self._get_name(node.value)}.{node.attr}"
        return None
    
    def _find_function_end(self, node):
        """
        Find the last line of a function by analyzing its AST subtree.
        
        This method uses a heuristic to determine where a function ends
        by finding the maximum line number of any node in its subtree.
        
        Args:
            node: AST FunctionDef node
            
        Returns:
            int: Line number where the function ends
        """
        # Simple heuristic - find the last line with content in the function
        max_line = node.lineno
        for child in ast.walk(node):
            if hasattr(child, 'lineno'):
                max_line = max(max_line, child.lineno)
        return min(max_line + 1, len(self.lines))
    
    def _find_class_end(self, node):
        """
        Find the last line of a class by analyzing its AST subtree.
        
        Similar to _find_function_end but for class definitions.
        
        Args:
            node: AST ClassDef node
            
        Returns:
            int: Line number where the class ends
        """
        max_line = node.lineno
        for child in ast.walk(node):
            if hasattr(child, 'lineno'):
                max_line = max(max_line, child.lineno)
        return min(max_line + 1, len(self.lines))
    
    def _has_return_statement(self, node):
        """
        Check if function has any return statements.
        
        This method walks the function's AST to determine if it contains
        any return statements, which is useful for semantic analysis.
        
        Args:
            node: AST FunctionDef node
            
        Returns:
            bool: True if the function has return statements, False otherwise
        """
        for child in ast.walk(node):
            if isinstance(child, ast.Return):
                return True
        return False

class ComplexityVisitor(ast.NodeVisitor):
    """
    AST visitor that computes cyclomatic complexity of code entities.
    
    This visitor implements a simplified cyclomatic complexity calculation
    by counting decision points in the code (if statements, loops, etc.).
    The complexity metric helps identify potentially problematic code areas.
    
    Complexity is calculated as:
    - Base complexity: 1
    - +1 for each if/elif statement
    - +1 for each for/while loop
    - +1 for each except handler
    - +1 for each boolean operator (and/or)
    
    Attributes:
        complexity (int): Current complexity score
    """
    
    def __init__(self):
        """Initialize complexity counter to 1 (base complexity)."""
        self.complexity = 1
    
    def visit_If(self, node):
        """Count if statements as decision points (+1 complexity)."""
        self.complexity += 1
        self.generic_visit(node)
    
    def visit_For(self, node):
        """Count for loops as decision points (+1 complexity)."""
        self.complexity += 1
        self.generic_visit(node)
    
    def visit_While(self, node):
        """Count while loops as decision points (+1 complexity)."""
        self.complexity += 1
        self.generic_visit(node)

class DependencyVisitor(ast.NodeVisitor):
    """
    AST visitor that identifies dependencies (variables/functions used).
    
    This visitor collects all names that are loaded (read) within a code
    entity, which helps identify dependencies and relationships between
    different parts of the code.
    
    Attributes:
        dependencies (List[str]): List of dependency names found
    """
    
    def __init__(self):
        """Initialize empty dependencies list."""
        self.dependencies = []
    
    def visit_Name(self, node):
        """
        Collect names that are loaded (dependencies).
        
        Only collects names in Load context (being read), not Store
        context (being assigned to).
        
        Args:
            node: AST Name node
        """
        if isinstance(node.ctx, ast.Load):
            self.dependencies.append(node.id)

# ===== 2. CODE EMBEDDING GENERATION =====

class CodeEmbeddingGenerator:
    """
    Generates embeddings for code entities using multiple approaches.
    
    This class creates vector representations of code entities by combining
    three different embedding techniques:
    1. Text-based embeddings: From source code and documentation
    2. Structure-based embeddings: From AST patterns and metrics
    3. Context-based embeddings: From semantic patterns and intent
    
    The hybrid approach provides richer representations that capture both
    syntactic and semantic aspects of code entities.
    
    In a production environment, this would integrate with models like:
    - CodeBERT for text embeddings
    - Code2Vec for structural embeddings
    - Custom models for contextual embeddings
    
    Attributes:
        model_name (str): Name of the base model for text embeddings
    """
    
    def __init__(self, model_name='microsoft/codebert-base'):
        """
        Initialize the embedding generator.
        
        Args:
            model_name (str): Model identifier for text-based embeddings
        """
        self.model_name = model_name
        # In practice, you'd load actual models here
        # self.text_model = AutoModel.from_pretrained(model_name)
        # self.tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    def generate_embeddings(self, entities: List[CodeEntity]) -> Dict[str, np.ndarray]:
        """
        Generate embeddings for all code entities.
        
        This method processes each entity through the complete embedding
        pipeline, generating and combining multiple types of embeddings
        into a single vector representation.
        
        Args:
            entities (List[CodeEntity]): List of entities to embed
            
        Returns:
            Dict[str, np.ndarray]: Mapping from entity IDs to embedding vectors
        """
        embeddings = {}
        
        for entity in entities:
            # Generate multiple types of embeddings
            text_embedding = self._generate_text_embedding(entity)
            structure_embedding = self._generate_structure_embedding(entity)
            context_embedding = self._generate_context_embedding(entity)
            
            # Combine embeddings
            combined_embedding = np.concatenate([
                text_embedding,
                structure_embedding,
                context_embedding
            ])
            
            embeddings[entity.id] = combined_embedding
        
        return embeddings
    
    def _generate_text_embedding(self, entity: CodeEntity) -> np.ndarray:
        """
        Generate text-based embedding from source code and docstring.
        
        This method would typically use a pre-trained code language model
        like CodeBERT, GraphCodeBERT, or similar to generate embeddings
        from the textual representation of the code.
        
        Args:
            entity (CodeEntity): Entity to generate embedding for
            
        Returns:
            np.ndarray: Text-based embedding vector
            
        Note:
            Current implementation uses a mock embedding for demonstration.
            In production, this would use actual transformer models.
        """
        text = f"{entity.name} {entity.docstring or ''} {entity.source_code}"
        
        # Simplified - in practice use actual CodeBERT/similar model
        # tokens = self.tokenizer(text, return_tensors='pt', truncation=True)
        # with torch.no_grad():
        #     outputs = self.text_model(**tokens)
        #     embedding = outputs.last_hidden_state.mean(dim=1).squeeze()
        
        # Mock embedding for demo - creates deterministic embedding based on content
        hash_val = int(hashlib.md5(text.encode()).hexdigest()[:8], 16)
        np.random.seed(hash_val)
        return np.random.random(384).astype(np.float32)
    
    def _generate_structure_embedding(self, entity: CodeEntity) -> np.ndarray:
        """
        Generate structure-based embedding from AST patterns.
        
        This method creates embeddings based on structural properties
        of the code entity such as complexity, size, and dependencies.
        In practice, this would use code2vec-style path embeddings.
        
        Args:
            entity (CodeEntity): Entity to generate embedding for
            
        Returns:
            np.ndarray: Structure-based embedding vector
        """
        # This would use code2vec-style path embeddings in practice
        features = [
            entity.complexity,
            len(entity.dependencies),
            entity.line_end - entity.line_start,
            len(entity.context.get('args', [])),
            1 if entity.docstring else 0
        ]
        
        # Pad to fixed size for consistent vector dimensions
        features.extend([0] * (64 - len(features)))
        return np.array(features[:64], dtype=np.float32)
    
    def _generate_context_embedding(self, entity: CodeEntity) -> np.ndarray:
        """
        Generate context-based embedding from semantic patterns.
        
        This method creates embeddings based on semantic context such as
        entity type, naming patterns, visibility, and inferred intent.
        These features help capture the semantic role of code entities.
        
        Args:
            entity (CodeEntity): Entity to generate embedding for
            
        Returns:
            np.ndarray: Context-based embedding vector
        """
        context_features = []
        
        # Entity type encoding
        type_map = {'function': 0, 'method': 1, 'class': 2, 'variable': 3}
        context_features.append(type_map.get(entity.type, 0))
        
        # Semantic intent encoding
        intent_map = {'data_access': 0, 'data_modification': 1, 'computation': 2, 'control': 3}
        intent = entity.context.get('intent', 'computation')
        context_features.append(intent_map.get(intent, 2))
        
        # Complexity level
        complexity_map = {'low': 0, 'medium': 1, 'high': 2}
        complexity_level = entity.context.get('complexity_level', 'medium')
        context_features.append(complexity_map.get(complexity_level, 1))
        
        # Visibility
        visibility_map = {'public': 0, 'private': 1, 'protected': 2}
        visibility = entity.context.get('visibility', 'public')
        context_features.append(visibility_map.get(visibility, 0))
        
        # Pad to fixed size
        context_features.extend([0] * (32 - len(context_features)))
        return np.array(context_features[:32], dtype=np.float32)

# ===== 3. KNOWLEDGE GRAPH CONSTRUCTION =====

class CodeKnowledgeGraph:
    """
    Builds and manages a knowledge graph of code entities and relationships.
    
    This class provides a hybrid storage and query system that combines:
    1. Relational database (SQLite) for structured entity and relationship data
    2. Vector storage for embedding-based semantic search
    3. Graph traversal capabilities for relationship discovery
    
    The knowledge graph enables multiple types of queries:
    - Semantic similarity search using vector embeddings
    - Graph traversal for finding related entities
    - Structured queries for filtering by properties
    - Hybrid search combining multiple approaches
    
    Database Schema:
    - entities: Stores code entities with metadata and embeddings
    - relations: Stores relationships between entities
    
    Attributes:
        db_path (str): Path to the SQLite database file
    """
    
    def __init__(self, db_path: str = "code_knowledge.db"):
        """
        Initialize the knowledge graph with database setup.
        
        Args:
            db_path (str): Path where the SQLite database will be stored
        """
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """
        Initialize SQLite database schema for knowledge graph.
        
        Creates tables for entities and relations with appropriate indexes
        for efficient querying. The schema supports:
        - Full-text search on entity names and types
        - Vector similarity search on embeddings
        - Graph traversal queries on relationships
        """
        conn = sqlite3.connect(self.db_path)
        
        # Entities table
        conn.execute('''
            CREATE TABLE IF NOT EXISTS entities (
                id TEXT PRIMARY KEY,
                type TEXT,
                name TEXT,
                full_name TEXT,
                file_path TEXT,
                line_start INTEGER,
                line_end INTEGER,
                complexity INTEGER,
                docstring TEXT,
                source_code TEXT,
                context JSON,
                embedding BLOB,
                business_domain TEXT,
                architectural_pattern TEXT,
                team_ownership TEXT,
                importance_score REAL,
                contextual_annotations JSON,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Relations table
        conn.execute('''
            CREATE TABLE IF NOT EXISTS relations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_id TEXT,
                target_id TEXT,
                relation_type TEXT,
                confidence REAL,
                context JSON,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (source_id) REFERENCES entities (id),
                FOREIGN KEY (target_id) REFERENCES entities (id)
            )
        ''')
        
        # Create indexes for performance
        conn.execute('CREATE INDEX IF NOT EXISTS idx_entity_type ON entities(type)')
        conn.execute('CREATE INDEX IF NOT EXISTS idx_entity_name ON entities(name)')
        conn.execute('CREATE INDEX IF NOT EXISTS idx_business_domain ON entities(business_domain)')
        conn.execute('CREATE INDEX IF NOT EXISTS idx_team_ownership ON entities(team_ownership)')
        conn.execute('CREATE INDEX IF NOT EXISTS idx_importance_score ON entities(importance_score)')
        conn.execute('CREATE INDEX IF NOT EXISTS idx_relation_type ON relations(relation_type)')
        conn.execute('CREATE INDEX IF NOT EXISTS idx_source_target ON relations(source_id, target_id)')
        
        conn.commit()
        conn.close()
    
    def store_entities_and_embeddings(self, entities: List[CodeEntity], 
                                    embeddings: Dict[str, np.ndarray]):
        """
        Store entities and their embeddings in the knowledge graph.
        
        This method persists code entities along with their vector embeddings
        to the database. Embeddings are stored as binary data for efficient
        retrieval during similarity search operations.
        
        Args:
            entities (List[CodeEntity]): List of entities to store
            embeddings (Dict[str, np.ndarray]): Mapping from entity IDs to embeddings
            
        Note:
            Uses INSERT OR REPLACE to handle updates to existing entities
        """
        conn = sqlite3.connect(self.db_path)
        
        for entity in entities:
            embedding_blob = embeddings[entity.id].tobytes() if entity.id in embeddings else None
            
            conn.execute('''
                INSERT OR REPLACE INTO entities 
                (id, type, name, full_name, file_path, line_start, line_end, 
                 complexity, docstring, source_code, context, embedding,
                 business_domain, architectural_pattern, team_ownership,
                 importance_score, contextual_annotations)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                entity.id, entity.type, entity.name, entity.full_name,
                entity.file_path, entity.line_start, entity.line_end,
                entity.complexity, entity.docstring, entity.source_code,
                json.dumps(entity.context), embedding_blob,
                entity.business_domain, entity.architectural_pattern,
                entity.team_ownership, entity.importance_score,
                json.dumps(entity.contextual_annotations) if entity.contextual_annotations else None
            ))
        
        conn.commit()
        conn.close()
    
    def store_relations(self, relations: List[CodeRelation]):
        """
        Store relationships between entities.
        
        This method persists relationship data that connects entities
        in the knowledge graph. Relationships enable graph traversal
        and discovery of related code elements.
        
        Args:
            relations (List[CodeRelation]): List of relationships to store
        """
        conn = sqlite3.connect(self.db_path)
        
        for relation in relations:
            conn.execute('''
                INSERT OR REPLACE INTO relations
                (source_id, target_id, relation_type, confidence, context)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                relation.source_id, relation.target_id, relation.relation_type,
                relation.confidence, json.dumps(relation.context)
            ))
        
        conn.commit()
        conn.close()
    
    def semantic_search(self, query_embedding: np.ndarray, top_k: int = 10) -> List[Dict]:
        """
        Perform semantic search using vector similarity.
        
        This method finds entities that are semantically similar to a query
        by comparing vector embeddings using cosine similarity. It's useful
        for finding code that performs similar functions or has similar
        characteristics.
        
        Args:
            query_embedding (np.ndarray): Query vector to search for
            top_k (int): Maximum number of results to return
            
        Returns:
            List[Dict]: List of similar entities with similarity scores
            
        Note:
            Uses cosine similarity metric for comparing embeddings
        """
        conn = sqlite3.connect(self.db_path)
        
        cursor = conn.execute('SELECT id, name, type, embedding FROM entities WHERE embedding IS NOT NULL')
        
        results = []
        for row in cursor:
            entity_id, name, entity_type, embedding_blob = row
            
            # Deserialize embedding
            entity_embedding = np.frombuffer(embedding_blob, dtype=np.float32)
            
            # Calculate cosine similarity
            similarity = np.dot(query_embedding, entity_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(entity_embedding)
            )
            
            results.append({
                'id': entity_id,
                'name': name,
                'type': entity_type,
                'similarity': similarity
            })
        
        # Sort by similarity and return top-k
        results.sort(key=lambda x: x['similarity'], reverse=True)
        conn.close()
        
        return results[:top_k]
    
    def graph_search(self, entity_id: str, relation_types: List[str] = None, 
                    depth: int = 2) -> Dict:
        """
        Perform graph traversal search starting from an entity.
        
        This method explores the relationship graph starting from a specific
        entity and returns connected entities within a given depth. It's
        useful for understanding code dependencies and impact analysis.
        
        Args:
            entity_id (str): Starting entity for traversal
            relation_types (List[str], optional): Filter by specific relation types
            depth (int): Maximum traversal depth
            
        Returns:
            Dict: Graph structure with nodes and edges
            
        Note:
            Uses breadth-first traversal with cycle detection
        """
        conn = sqlite3.connect(self.db_path)
        
        visited = set()
        result = {'nodes': [], 'edges': []}
        
        def traverse(current_id, current_depth):
            if current_depth > depth or current_id in visited:
                return
            
            visited.add(current_id)
            
            # Get entity info
            cursor = conn.execute(
                'SELECT * FROM entities WHERE id = ?', (current_id,)
            )
            entity = cursor.fetchone()
            if entity:
                result['nodes'].append({
                    'id': entity[0],
                    'type': entity[1],
                    'name': entity[2],
                    'complexity': entity[6]
                })
            
            # Get related entities
            relation_filter = ''
            params = [current_id]
            
            if relation_types:
                placeholders = ','.join(['?'] * len(relation_types))
                relation_filter = f' AND relation_type IN ({placeholders})'
                params.extend(relation_types)
            
            cursor = conn.execute(f'''
                SELECT target_id, relation_type, confidence 
                FROM relations 
                WHERE source_id = ?{relation_filter}
            ''', params)
            
            for target_id, relation_type, confidence in cursor:
                result['edges'].append({
                    'source': current_id,
                    'target': target_id,
                    'type': relation_type,
                    'confidence': confidence
                })
                
                traverse(target_id, current_depth + 1)
        
        traverse(entity_id, 0)
        conn.close()
        
        return result
    
    def contextual_search(self, query: str, context_filters: Dict[str, Any] = None, 
                         top_k: int = 10) -> List[Dict]:
        """
        Perform contextual search considering business domains, teams, and importance.
        
        This method provides enhanced search that takes into account:
        - Business domain relevance
        - Team ownership
        - Importance scores
        - Contextual annotations
        
        Args:
            query (str): Search query
            context_filters (Dict[str, Any], optional): Context-based filters
            top_k (int): Maximum number of results
            
        Returns:
            List[Dict]: Contextually ranked search results
        """
        conn = sqlite3.connect(self.db_path)
        
        # Build the base query
        sql_query = '''
            SELECT id, name, type, business_domain, team_ownership, 
                   importance_score, contextual_annotations, embedding,
                   file_path, complexity
            FROM entities 
            WHERE embedding IS NOT NULL
        '''
        
        params = []
        
        # Apply context filters
        if context_filters:
            if 'business_domain' in context_filters:
                sql_query += ' AND business_domain = ?'
                params.append(context_filters['business_domain'])
            
            if 'team_ownership' in context_filters:
                sql_query += ' AND team_ownership = ?'
                params.append(context_filters['team_ownership'])
            
            if 'min_importance' in context_filters:
                sql_query += ' AND importance_score >= ?'
                params.append(context_filters['min_importance'])
            
            if 'architectural_pattern' in context_filters:
                sql_query += ' AND architectural_pattern = ?'
                params.append(context_filters['architectural_pattern'])
            
            if 'has_security_annotations' in context_filters and context_filters['has_security_annotations']:
                sql_query += ' AND contextual_annotations LIKE ?'
                params.append('%security%')
        
        cursor = conn.execute(sql_query, params)
        
        # Generate query embedding for semantic similarity
        query_entity = CodeEntity(
            id="query", type="query", name=query, full_name=query,
            source_code=query, docstring=None, file_path="",
            line_start=0, line_end=0, complexity=0,
            dependencies=[], usages=[], context={},
            contextual_annotations=[]
        )
        
        # This would use the actual embedding generator
        query_embedding = np.random.random(480).astype(np.float32)  # Mock for demo
        
        results = []
        for row in cursor:
            (entity_id, name, entity_type, business_domain, team_ownership,
             importance_score, contextual_annotations, embedding_blob,
             file_path, complexity) = row
            
            # Calculate semantic similarity
            if embedding_blob:
                entity_embedding = np.frombuffer(embedding_blob, dtype=np.float32)
                similarity = np.dot(query_embedding, entity_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(entity_embedding)
                )
            else:
                similarity = 0.0
            
            # Calculate contextual score
            contextual_score = similarity * importance_score
            
            # Parse annotations
            annotations = []
            if contextual_annotations:
                try:
                    annotations = json.loads(contextual_annotations)
                except:
                    annotations = []
            
            results.append({
                'id': entity_id,
                'name': name,
                'type': entity_type,
                'similarity': similarity,
                'importance_score': importance_score,
                'contextual_score': contextual_score,
                'business_domain': business_domain,
                'team_ownership': team_ownership,
                'annotations': annotations,
                'file_path': file_path,
                'complexity': complexity
            })
        
        # Sort by contextual score (similarity * importance)
        results.sort(key=lambda x: x['contextual_score'], reverse=True)
        conn.close()
        
        return results[:top_k]
    
    def get_domain_analytics(self) -> Dict[str, Any]:
        """
        Generate analytics about business domains in the codebase.
        
        Returns:
            Dict[str, Any]: Domain analytics including entity counts and complexity
        """
        conn = sqlite3.connect(self.db_path)
        
        cursor = conn.execute('''
            SELECT business_domain, COUNT(*) as entity_count,
                   AVG(complexity) as avg_complexity,
                   AVG(importance_score) as avg_importance,
                   COUNT(CASE WHEN contextual_annotations IS NOT NULL THEN 1 END) as annotated_count
            FROM entities 
            WHERE business_domain IS NOT NULL
            GROUP BY business_domain
            ORDER BY entity_count DESC
        ''')
        
        domains = {}
        for row in cursor:
            domain, count, avg_complexity, avg_importance, annotated_count = row
            domains[domain] = {
                'entity_count': count,
                'avg_complexity': round(avg_complexity, 2) if avg_complexity else 0,
                'avg_importance': round(avg_importance, 2) if avg_importance else 1.0,
                'annotated_entities': annotated_count
            }
        
        conn.close()
        return domains
    
    def get_team_analytics(self) -> Dict[str, Any]:
        """
        Generate analytics about team ownership in the codebase.
        
        Returns:
            Dict[str, Any]: Team analytics including ownership and quality metrics
        """
        conn = sqlite3.connect(self.db_path)
        
        cursor = conn.execute('''
            SELECT team_ownership, COUNT(*) as entity_count,
                   AVG(complexity) as avg_complexity,
                   COUNT(CASE WHEN context LIKE '%quality_issues%' THEN 1 END) as quality_issues_count,
                   COUNT(CASE WHEN context LIKE '%style_issues%' THEN 1 END) as style_issues_count
            FROM entities 
            WHERE team_ownership IS NOT NULL
            GROUP BY team_ownership
            ORDER BY entity_count DESC
        ''')
        
        teams = {}
        for row in cursor:
            team, count, avg_complexity, quality_issues, style_issues = row
            teams[team] = {
                'entity_count': count,
                'avg_complexity': round(avg_complexity, 2) if avg_complexity else 0,
                'quality_issues_count': quality_issues,
                'style_issues_count': style_issues,
                'quality_score': round((count - quality_issues - style_issues) / count * 100, 1) if count > 0 else 100
            }
        
        conn.close()
        return teams

# ===== 4. AI AGENT INTERFACE SYSTEM =====

@dataclass
class QueryResult:
    """
    Represents the result of an AI agent query.
    
    Attributes:
        query (str): Original query text
        intent (str): Detected query intent
        entities (List[Dict]): Relevant code entities
        context (Dict[str, Any]): Contextual information
        suggestions (List[str]): Follow-up suggestions
        confidence (float): Confidence score for the results
        execution_time (float): Query execution time in seconds
    """
    query: str
    intent: str
    entities: List[Dict[str, Any]]
    context: Dict[str, Any]
    suggestions: List[str]
    confidence: float
    execution_time: float

class QueryIntent:
    """Enumeration of supported query intents."""
    FIND_FUNCTION = "find_function"
    FIND_CLASS = "find_class"
    SECURITY_ANALYSIS = "security_analysis"
    PERFORMANCE_ANALYSIS = "performance_analysis"
    DEPENDENCY_ANALYSIS = "dependency_analysis"
    TEAM_OWNERSHIP = "team_ownership"
    DOMAIN_ANALYSIS = "domain_analysis"
    ARCHITECTURAL_ANALYSIS = "architectural_analysis"
    CODE_QUALITY = "code_quality"
    SIMILAR_CODE = "similar_code"
    IMPACT_ANALYSIS = "impact_analysis"
    DOCUMENTATION = "documentation"
    CODE_OVERVIEW = "code_overview"

class AIQueryProcessor(ABC):
    """Abstract base class for AI query processors."""
    
    @abstractmethod
    def can_handle(self, query: str) -> bool:
        """Check if this processor can handle the given query."""
        pass
    
    @abstractmethod
    def process(self, query: str, knowledge_graph: 'CodeKnowledgeGraph') -> QueryResult:
        """Process the query and return results."""
        pass

class NaturalLanguageQueryProcessor(AIQueryProcessor):
    """
    Processes natural language queries and converts them to knowledge graph operations.
    
    This processor analyzes natural language queries to understand user intent
    and translates them into appropriate knowledge graph queries.
    """
    
    def __init__(self):
        """Initialize the natural language processor with intent patterns."""
        self.intent_patterns = {
            QueryIntent.FIND_FUNCTION: [
                r"find.*function.*(?:named|called)\s+(\w+)",
                r"show.*function.*(\w+)",
                r"where.*function.*(\w+)",
                r"function.*(\w+).*definition",
                r"(?:find|show|locate).*(\w+).*function"
            ],
            QueryIntent.FIND_CLASS: [
                r"find.*class.*(?:named|called)\s+(\w+)",
                r"show.*class.*(\w+)",
                r"where.*class.*(\w+)",
                r"class.*(\w+).*definition",
                r"(?:find|show|locate).*(\w+).*class"
            ],
            QueryIntent.SECURITY_ANALYSIS: [
                r"security.*(?:issues|problems|vulnerabilities)",
                r"(?:find|show).*security.*(?:critical|sensitive)",
                r"authentication.*(?:code|functions)",
                r"authorization.*(?:code|functions)",
                r"(?:password|token|key).*handling",
                r"security.*annotations"
            ],
            QueryIntent.PERFORMANCE_ANALYSIS: [
                r"performance.*(?:issues|problems|bottlenecks)",
                r"slow.*(?:functions|code|methods)",
                r"(?:find|show).*performance.*critical",
                r"latency.*(?:issues|problems)",
                r"optimization.*(?:opportunities|candidates)"
            ],
            QueryIntent.DEPENDENCY_ANALYSIS: [
                r"(?:dependencies|depends).*(?:of|for)\s+(\w+)",
                r"what.*uses.*(\w+)",
                r"what.*depends.*(\w+)",
                r"(?:find|show).*dependencies",
                r"impact.*(?:of|from).*(\w+)"
            ],
            QueryIntent.TEAM_OWNERSHIP: [
                r"who.*owns.*(\w+)",
                r"(?:team|owner).*(?:of|for).*(\w+)",
                r"contact.*(?:for|about).*(\w+)",
                r"responsible.*(?:for|team)"
            ],
            QueryIntent.DOMAIN_ANALYSIS: [
                r"(?:show|find).*(\w+).*domain.*code",
                r"business.*domain.*(\w+)",
                r"domain.*analysis.*(\w+)",
                r"(\w+).*domain.*entities"
            ],
            QueryIntent.ARCHITECTURAL_ANALYSIS: [
                r"architectural.*patterns",
                r"(?:mvc|repository|factory).*pattern",
                r"design.*patterns",
                r"architecture.*(?:analysis|overview)"
            ],
            QueryIntent.CODE_QUALITY: [
                r"code.*quality.*(?:issues|problems)",
                r"(?:complex|complicated).*(?:functions|code)",
                r"quality.*(?:metrics|scores)",
                r"(?:style|lint).*(?:issues|violations)"
            ],
            QueryIntent.SIMILAR_CODE: [
                r"similar.*(?:to|like).*(\w+)",
                r"(?:find|show).*similar.*code",
                r"duplicate.*(?:functions|code)",
                r"related.*(?:functions|code)"
            ],
            QueryIntent.DOCUMENTATION: [
                r"(?:show|find).*documentation.*(\w+)",
                r"docstring.*(?:for|of).*(\w+)",
                r"(?:help|docs).*(?:for|about).*(\w+)",
                r"explain.*(\w+).*(?:function|class|method)"
            ],
            QueryIntent.CODE_OVERVIEW: [
                r"what.*does.*(?:this|the).*code.*do",
                r"what.*is.*(?:this|the).*code.*for",
                r"explain.*(?:this|the).*(?:code|codebase|system|project)",
                r"describe.*(?:this|the).*(?:code|codebase|system|project)",
                r"overview.*of.*(?:this|the).*(?:code|codebase|system|project)",
                r"summary.*of.*(?:this|the).*(?:code|codebase|system|project)",
                r"what.*does.*(?:this|the).*(?:project|codebase|system).*do",
                r"purpose.*of.*(?:this|the).*(?:code|codebase|system|project)",
                r"functionality.*of.*(?:this|the).*(?:code|codebase|system|project)",
                r"what.*(?:is|does).*(?:this|the).*(?:codebase|system|project)"
            ]
        }
        
        self.domain_keywords = {
            "authentication": ["auth", "login", "user", "password", "token", "session"],
            "payment": ["payment", "billing", "charge", "transaction", "invoice"],
            "database": ["db", "sql", "query", "model", "table", "migration"],
            "api": ["api", "endpoint", "route", "handler", "request", "response"],
            "security": ["security", "encrypt", "hash", "validate", "sanitize"],
            "performance": ["performance", "cache", "optimize", "latency", "speed"]
        }
    
    def can_handle(self, query: str) -> bool:
        """Check if this is a natural language query."""
        # Simple heuristic: if query contains common English words and is longer than a single term
        english_indicators = ["find", "show", "what", "where", "who", "how", "is", "are", "the", "in", "of", "for"]
        words = query.lower().split()
        return len(words) > 1 and any(indicator in words for indicator in english_indicators)
    
    def process(self, query: str, knowledge_graph: 'CodeKnowledgeGraph') -> QueryResult:
        """Process natural language query and return structured results."""
        import time
        start_time = time.time()
        
        # Detect intent
        intent, extracted_terms = self._detect_intent(query)
        
        # Execute appropriate query based on intent
        if intent == QueryIntent.FIND_FUNCTION:
            entities = self._find_entities_by_name(knowledge_graph, extracted_terms, entity_type="function")
        elif intent == QueryIntent.FIND_CLASS:
            entities = self._find_entities_by_name(knowledge_graph, extracted_terms, entity_type="class")
        elif intent == QueryIntent.SECURITY_ANALYSIS:
            entities = self._find_security_critical_code(knowledge_graph)
        elif intent == QueryIntent.PERFORMANCE_ANALYSIS:
            entities = self._find_performance_critical_code(knowledge_graph)
        elif intent == QueryIntent.DEPENDENCY_ANALYSIS:
            entities = self._find_dependencies(knowledge_graph, extracted_terms)
        elif intent == QueryIntent.TEAM_OWNERSHIP:
            entities = self._find_by_team_ownership(knowledge_graph, extracted_terms)
        elif intent == QueryIntent.DOMAIN_ANALYSIS:
            entities = self._find_by_domain(knowledge_graph, extracted_terms)
        elif intent == QueryIntent.ARCHITECTURAL_ANALYSIS:
            entities = self._find_by_architectural_pattern(knowledge_graph, extracted_terms)
        elif intent == QueryIntent.CODE_QUALITY:
            entities = self._find_quality_issues(knowledge_graph)
        elif intent == QueryIntent.SIMILAR_CODE:
            entities = self._find_similar_code(knowledge_graph, query)
        elif intent == QueryIntent.DOCUMENTATION:
            entities = self._find_documented_code(knowledge_graph, extracted_terms)
        elif intent == QueryIntent.CODE_OVERVIEW:
            entities = self._generate_code_overview(knowledge_graph)
        else:
            # Fallback to semantic search
            entities = knowledge_graph.contextual_search(query, top_k=10)
        
        # Generate context and suggestions
        context = self._generate_context(entities, intent, query)
        suggestions = self._generate_suggestions(intent, entities, query)
        confidence = self._calculate_confidence(entities, intent, query)
        
        execution_time = time.time() - start_time
        
        return QueryResult(
            query=query,
            intent=intent,
            entities=entities,
            context=context,
            suggestions=suggestions,
            confidence=confidence,
            execution_time=execution_time
        )
    
    def _detect_intent(self, query: str) -> Tuple[str, List[str]]:
        """Detect query intent and extract relevant terms."""
        query_lower = query.lower()
        
        for intent, patterns in self.intent_patterns.items():
            for pattern in patterns:
                match = re.search(pattern, query_lower)
                if match:
                    # Extract captured groups as relevant terms
                    terms = [group for group in match.groups() if group]
                    return intent, terms
        
        # If no specific intent detected, extract domain-related terms
        extracted_terms = []
        for domain, keywords in self.domain_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                extracted_terms.append(domain)
        
        return QueryIntent.SIMILAR_CODE, extracted_terms  # Default fallback
    
    def _find_entities_by_name(self, kg: 'CodeKnowledgeGraph', terms: List[str], entity_type: str = None) -> List[Dict]:
        """Find entities by name or partial name match."""
        if not terms:
            return []
        
        conn = sqlite3.connect(kg.db_path)
        
        conditions = []
        params = []
        
        for term in terms:
            conditions.append("(name LIKE ? OR full_name LIKE ?)")
            params.extend([f"%{term}%", f"%{term}%"])
        
        if entity_type:
            conditions.append("type = ?")
            params.append(entity_type)
        
        where_clause = " AND ".join(conditions)
        
        cursor = conn.execute(f"""
            SELECT id, name, type, file_path, complexity, business_domain, 
                   team_ownership, importance_score, docstring
            FROM entities 
            WHERE {where_clause}
            ORDER BY importance_score DESC
            LIMIT 15
        """, params)
        
        results = []
        for row in cursor:
            results.append({
                'id': row[0],
                'name': row[1],
                'type': row[2],
                'file_path': row[3],
                'complexity': row[4],
                'business_domain': row[5],
                'team_ownership': row[6],
                'importance_score': row[7],
                'docstring': row[8]
            })
        
        conn.close()
        return results
    
    def _find_security_critical_code(self, kg: 'CodeKnowledgeGraph') -> List[Dict]:
        """Find security-critical code entities."""
        return kg.contextual_search("security authentication", {
            'has_security_annotations': True,
            'min_importance': 1.5
        })
    
    def _find_performance_critical_code(self, kg: 'CodeKnowledgeGraph') -> List[Dict]:
        """Find performance-critical code entities."""
        conn = sqlite3.connect(kg.db_path)
        
        cursor = conn.execute("""
            SELECT id, name, type, file_path, complexity, business_domain,
                   team_ownership, importance_score, contextual_annotations
            FROM entities 
            WHERE (contextual_annotations LIKE '%performance%' 
                   OR contextual_annotations LIKE '%critical_path%'
                   OR complexity > 10)
            ORDER BY importance_score DESC, complexity DESC
            LIMIT 15
        """)
        
        results = []
        for row in cursor:
            results.append({
                'id': row[0],
                'name': row[1],
                'type': row[2],
                'file_path': row[3],
                'complexity': row[4],
                'business_domain': row[5],
                'team_ownership': row[6],
                'importance_score': row[7],
                'annotations': row[8]
            })
        
        conn.close()
        return results
    
    def _find_dependencies(self, kg: 'CodeKnowledgeGraph', terms: List[str]) -> List[Dict]:
        """Find dependencies for specified entities."""
        if not terms:
            return []
        
        # First find the entities mentioned in terms
        entities = self._find_entities_by_name(kg, terms)
        
        results = []
        for entity in entities[:5]:  # Limit to first 5 matches
            # Get dependencies using graph search
            graph_result = kg.graph_search(entity['id'], ['depends_on', 'calls'], depth=2)
            
            entity['dependencies'] = graph_result
            results.append(entity)
        
        return results
    
    def _find_by_team_ownership(self, kg: 'CodeKnowledgeGraph', terms: List[str]) -> List[Dict]:
        """Find entities by team ownership."""
        if not terms:
            return []
        
        conn = sqlite3.connect(kg.db_path)
        
        conditions = []
        params = []
        
        for term in terms:
            conditions.append("team_ownership LIKE ?")
            params.append(f"%{term}%")
        
        where_clause = " OR ".join(conditions)
        
        cursor = conn.execute(f"""
            SELECT id, name, type, file_path, complexity, business_domain,
                   team_ownership, importance_score
            FROM entities 
            WHERE {where_clause}
            ORDER BY importance_score DESC
            LIMIT 20
        """, params)
        
        results = []
        for row in cursor:
            results.append({
                'id': row[0],
                'name': row[1],
                'type': row[2],
                'file_path': row[3],
                'complexity': row[4],
                'business_domain': row[5],
                'team_ownership': row[6],
                'importance_score': row[7]
            })
        
        conn.close()
        return results
    
    def _find_by_domain(self, kg: 'CodeKnowledgeGraph', terms: List[str]) -> List[Dict]:
        """Find entities by business domain."""
        domain_filters = {}
        for term in terms:
            if term.lower() in ['authentication', 'auth']:
                domain_filters['business_domain'] = 'Authentication'
            elif term.lower() in ['payment', 'billing']:
                domain_filters['business_domain'] = 'Payment'
            elif term.lower() in ['database', 'db']:
                domain_filters['business_domain'] = 'Database'
            elif term.lower() in ['api']:
                domain_filters['business_domain'] = 'API'
        
        if domain_filters:
            return kg.contextual_search(" ".join(terms), domain_filters)
        else:
            return kg.contextual_search(" ".join(terms))
    
    def _find_by_architectural_pattern(self, kg: 'CodeKnowledgeGraph', terms: List[str]) -> List[Dict]:
        """Find entities by architectural pattern."""
        conn = sqlite3.connect(kg.db_path)
        
        cursor = conn.execute("""
            SELECT architectural_pattern, COUNT(*) as count,
                   AVG(complexity) as avg_complexity
            FROM entities 
            WHERE architectural_pattern IS NOT NULL
            GROUP BY architectural_pattern
            ORDER BY count DESC
        """)
        
        patterns = []
        for row in cursor:
            patterns.append({
                'pattern': row[0],
                'entity_count': row[1],
                'avg_complexity': round(row[2], 2) if row[2] else 0
            })
        
        conn.close()
        return patterns
    
    def _find_quality_issues(self, kg: 'CodeKnowledgeGraph') -> List[Dict]:
        """Find code quality issues."""
        conn = sqlite3.connect(kg.db_path)
        
        cursor = conn.execute("""
            SELECT id, name, type, file_path, complexity, business_domain,
                   team_ownership, context
            FROM entities 
            WHERE (context LIKE '%quality_issues%' 
                   OR context LIKE '%style_issues%'
                   OR complexity > 15)
            ORDER BY complexity DESC
            LIMIT 20
        """)
        
        results = []
        for row in cursor:
            try:
                context = json.loads(row[7]) if row[7] else {}
            except:
                context = {}
            
            quality_issues = context.get('quality_issues', [])
            style_issues = context.get('style_issues', [])
            
            results.append({
                'id': row[0],
                'name': row[1],
                'type': row[2],
                'file_path': row[3],
                'complexity': row[4],
                'business_domain': row[5],
                'team_ownership': row[6],
                'quality_issues': quality_issues,
                'style_issues': style_issues
            })
        
        conn.close()
        return results
    
    def _find_similar_code(self, kg: 'CodeKnowledgeGraph', query: str) -> List[Dict]:
        """Find similar code using semantic search."""
        return kg.contextual_search(query, top_k=15)
    
    def _find_documented_code(self, kg: 'CodeKnowledgeGraph', terms: List[str]) -> List[Dict]:
        """Find well-documented code entities."""
        if terms:
            entities = self._find_entities_by_name(kg, terms)
            # Filter for entities with docstrings
            return [e for e in entities if e.get('docstring')]
        
        conn = sqlite3.connect(kg.db_path)
        
        cursor = conn.execute("""
            SELECT id, name, type, file_path, complexity, business_domain,
                   team_ownership, docstring
            FROM entities 
            WHERE docstring IS NOT NULL AND docstring != ''
            ORDER BY LENGTH(docstring) DESC
            LIMIT 15
        """)
        
        results = []
        for row in cursor:
            results.append({
                'id': row[0],
                'name': row[1],
                'type': row[2],
                'file_path': row[3],
                'complexity': row[4],
                'business_domain': row[5],
                'team_ownership': row[6],
                'docstring': row[7]
            })
        
        conn.close()
        return results
    
    def _generate_context(self, entities: List[Dict], intent: str, query: str) -> Dict[str, Any]:
        """Generate contextual information about the results."""
        context = {
            'total_results': len(entities),
            'intent': intent,
            'query_type': 'natural_language'
        }
        
        if entities:
            # Analyze domains
            domains = {}
            teams = {}
            complexities = []
            
            for entity in entities:
                if isinstance(entity, dict):
                    domain = entity.get('business_domain')
                    if domain:
                        domains[domain] = domains.get(domain, 0) + 1
                    
                    team = entity.get('team_ownership')
                    if team:
                        teams[team] = teams.get(team, 0) + 1
                    
                    complexity = entity.get('complexity', 0)
                    if complexity:
                        complexities.append(complexity)
            
            context['domains_found'] = domains
            context['teams_involved'] = teams
            if complexities:
                context['avg_complexity'] = round(sum(complexities) / len(complexities), 2)
                context['max_complexity'] = max(complexities)
        
        return context
    
    def _generate_suggestions(self, intent: str, entities: List[Dict], query: str) -> List[str]:
        """Generate follow-up suggestions based on results."""
        suggestions = []
        
        if intent == QueryIntent.FIND_FUNCTION and entities:
            suggestions.append("Show dependencies of this function")
            suggestions.append("Find similar functions")
            suggestions.append("Check who owns this code")
        
        elif intent == QueryIntent.SECURITY_ANALYSIS:
            suggestions.append("Show team responsible for security")
            suggestions.append("Find authentication patterns")
            suggestions.append("Check security compliance")
        
        elif intent == QueryIntent.PERFORMANCE_ANALYSIS:
            suggestions.append("Show complex functions")
            suggestions.append("Find optimization opportunities")
            suggestions.append("Check performance annotations")
        
        elif intent == QueryIntent.TEAM_OWNERSHIP:
            suggestions.append("Show team's code quality metrics")
            suggestions.append("Find team's most complex code")
            suggestions.append("Show team's architectural patterns")
        
        elif intent == QueryIntent.CODE_OVERVIEW:
            suggestions.append("Find the most complex functions")
            suggestions.append("Show security-critical code")
            suggestions.append("Analyze architectural patterns")
        
        else:
            suggestions.extend([
                "Try a more specific search",
                "Search by business domain",
                "Find by team ownership"
            ])
        
        return suggestions[:3]  # Limit to 3 suggestions
    
    def _calculate_confidence(self, entities: List[Dict], intent: str, query: str) -> float:
        """Calculate confidence score for the results."""
        if not entities:
            return 0.1
        
        base_confidence = 0.7
        
        # Boost confidence for specific intents with good matches
        if intent in [QueryIntent.FIND_FUNCTION, QueryIntent.FIND_CLASS] and entities:
            # Check if we found exact name matches
            query_terms = query.lower().split()
            for entity in entities[:3]:
                if isinstance(entity, dict) and entity.get('name'):
                    if any(term in entity['name'].lower() for term in query_terms):
                        base_confidence += 0.2
                        break
        
        # Boost for high-importance entities
        if entities and isinstance(entities[0], dict):
            avg_importance = sum(e.get('importance_score', 1.0) for e in entities[:5]) / min(5, len(entities))
            if avg_importance > 1.5:
                base_confidence += 0.1
        
        return min(1.0, base_confidence)
    
    def _generate_code_overview(self, kg: 'CodeKnowledgeGraph') -> List[Dict]:
        """Generate a comprehensive overview of what the codebase does."""
        conn = sqlite3.connect(kg.db_path)
        
        # Get statistics about the codebase
        cursor = conn.execute("""
            SELECT 
                COUNT(*) as total_entities,
                COUNT(CASE WHEN type = 'class' THEN 1 END) as class_count,
                COUNT(CASE WHEN type = 'function' THEN 1 END) as function_count,
                COUNT(CASE WHEN type = 'method' THEN 1 END) as method_count,
                AVG(complexity) as avg_complexity,
                COUNT(DISTINCT file_path) as file_count
            FROM entities
        """)
        stats = cursor.fetchone()
        
        # Get most important/complex entities for overview
        cursor = conn.execute("""
            SELECT id, name, type, file_path, complexity, business_domain,
                   team_ownership, importance_score, docstring
            FROM entities 
            WHERE docstring IS NOT NULL AND docstring != ''
            ORDER BY importance_score DESC, complexity DESC
            LIMIT 10
        """)
        
        key_entities = []
        for row in cursor:
            key_entities.append({
                'id': row[0],
                'name': row[1],
                'type': row[2],
                'file_path': row[3],
                'complexity': row[4],
                'business_domain': row[5],
                'team_ownership': row[6],
                'importance_score': row[7],
                'docstring': row[8]
            })
        
        # Get domain distribution
        cursor = conn.execute("""
            SELECT business_domain, COUNT(*) as count
            FROM entities 
            WHERE business_domain IS NOT NULL
            GROUP BY business_domain
            ORDER BY count DESC
            LIMIT 5
        """)
        domains = [{'domain': row[0], 'count': row[1]} for row in cursor]
        
        # Get file types and structure
        cursor = conn.execute("""
            SELECT file_path, COUNT(*) as entity_count
            FROM entities
            GROUP BY file_path
            ORDER BY entity_count DESC
            LIMIT 10
        """)
        main_files = [{'file': row[0], 'entity_count': row[1]} for row in cursor]
        
        conn.close()
        
        # Create overview summary
        overview = {
            'type': 'overview',
            'total_entities': stats[0] if stats else 0,
            'class_count': stats[1] if stats else 0,
            'function_count': stats[2] if stats else 0,
            'method_count': stats[3] if stats else 0,
            'avg_complexity': round(stats[4], 2) if stats and stats[4] else 0,
            'file_count': stats[5] if stats else 0,
            'key_entities': key_entities,
            'domains': domains,
            'main_files': main_files,
            'summary': self._generate_codebase_description(stats, key_entities, domains, main_files)
        }
        
        return [overview]
    
    def _generate_codebase_description(self, stats, key_entities, domains, main_files) -> str:
        """Generate a natural language description of what the codebase does."""
        if not stats or stats[0] == 0:
            return "This appears to be an empty or unanalyzed codebase."
        
        description_parts = []
        
        # Basic statistics
        total_entities = stats[0]
        class_count = stats[1]
        function_count = stats[2]
        method_count = stats[3]
        file_count = stats[5]
        
        description_parts.append(f"This codebase contains {total_entities} code entities across {file_count} Python files.")
        
        if class_count > 0:
            description_parts.append(f"It includes {class_count} classes with {method_count} methods, plus {function_count} standalone functions.")
        else:
            description_parts.append(f"It consists primarily of {function_count} functions organized in a functional programming style.")
        
        # Analyze key components based on docstrings and names
        if key_entities:
            key_components = []
            for entity in key_entities[:5]:
                if entity.get('docstring'):
                    # Extract first sentence of docstring as purpose
                    first_sentence = entity['docstring'].split('.')[0] + '.'
                    if len(first_sentence) > 100:
                        first_sentence = first_sentence[:100] + "..."
                    key_components.append(f"• {entity['name']} ({entity['type']}): {first_sentence}")
                else:
                    key_components.append(f"• {entity['name']} ({entity['type']})")
            
            if key_components:
                description_parts.append("Key components include:")
                description_parts.extend(key_components)
        
        # Domain analysis
        if domains:
            domain_list = [f"{d['domain']} ({d['count']} entities)" for d in domains]
            description_parts.append(f"The code is organized around these business domains: {', '.join(domain_list)}.")
        
        # Architecture insights based on file structure
        if main_files:
            main_file_names = [f.split('/')[-1] for f in [mf['file'] for mf in main_files[:3]]]
            if any('test' in name.lower() for name in main_file_names):
                description_parts.append("The codebase includes comprehensive testing.")
            if any(name.lower() in ['main.py', '__main__.py', 'app.py', 'server.py'] for name in main_file_names):
                description_parts.append("This appears to be an application with a main entry point.")
            if any(name.lower() in ['pipeline.py', 'processor.py', 'analyzer.py'] for name in main_file_names):
                description_parts.append("The codebase implements data processing or analysis pipelines.")
        
        return " ".join(description_parts)

class CodebaseAIAgent:
    """
    AI Agent interface for querying the semantic knowledge graph.
    
    This agent provides a natural language interface to the codebase knowledge,
    allowing users to ask questions and get contextual answers about their code.
    """
    
    def __init__(self, knowledge_graph: CodeKnowledgeGraph):
        """
        Initialize the AI agent with a knowledge graph.
        
        Args:
            knowledge_graph (CodeKnowledgeGraph): The semantic knowledge graph to query
        """
        self.knowledge_graph = knowledge_graph
        self.processors = [
            NaturalLanguageQueryProcessor()
        ]
        self.query_history: List[QueryResult] = []
    
    def query(self, query_text: str) -> QueryResult:
        """
        Process a natural language query about the codebase.
        
        Args:
            query_text (str): Natural language query
            
        Returns:
            QueryResult: Structured result with entities and context
        """
        # Find appropriate processor
        processor = None
        for p in self.processors:
            if p.can_handle(query_text):
                processor = p
                break
        
        if not processor:
            # Fallback to basic search
            processor = self.processors[0]  # Use NL processor as fallback
        
        # Process the query
        result = processor.process(query_text, self.knowledge_graph)
        
        # Store in history
        self.query_history.append(result)
        
        return result
    
    def format_response(self, result: QueryResult, format_type: str = "natural") -> str:
        """
        Format the query result for presentation.
        
        Args:
            result (QueryResult): Query result to format
            format_type (str): Format type ('natural', 'structured', 'json')
            
        Returns:
            str: Formatted response
        """
        if format_type == "json":
            return json.dumps(asdict(result), indent=2, default=str)
        
        elif format_type == "structured":
            return self._format_structured(result)
        
        else:  # natural format
            return self._format_natural(result)
    
    def _format_natural(self, result: QueryResult) -> str:
        """Format result in natural language."""
        response = []
        
        # Add query acknowledgment
        response.append(f"🔍 Query: {result.query}")
        response.append(f"🎯 Intent: {result.intent.replace('_', ' ').title()}")
        response.append("")
        
        if not result.entities:
            response.append("❌ No relevant code entities found.")
            if result.suggestions:
                response.append("\n💡 Suggestions:")
                for suggestion in result.suggestions:
                    response.append(f"   • {suggestion}")
            return "\n".join(response)
        
        # Format entities based on intent
        if result.intent == QueryIntent.CODE_OVERVIEW:
            if result.entities and result.entities[0].get('type') == 'overview':
                overview = result.entities[0]
                response.append("📋 Codebase Overview:")
                response.append("")
                response.append(f"📊 Statistics:")
                response.append(f"   • Total Entities: {overview.get('total_entities', 0)}")
                response.append(f"   • Classes: {overview.get('class_count', 0)}")
                response.append(f"   • Functions: {overview.get('function_count', 0)}")
                response.append(f"   • Methods: {overview.get('method_count', 0)}")
                response.append(f"   • Files: {overview.get('file_count', 0)}")
                response.append(f"   • Average Complexity: {overview.get('avg_complexity', 0)}")
                response.append("")
                
                if overview.get('summary'):
                    response.append("🎯 What This Code Does:")
                    summary_lines = overview['summary'].split('. ')
                    for line in summary_lines:
                        if line.strip():
                            response.append(f"   {line.strip()}{'.' if not line.endswith('.') else ''}")
                    response.append("")
                
                if overview.get('key_entities'):
                    response.append("🔧 Key Components:")
                    for entity in overview['key_entities'][:5]:
                        response.append(f"   • {entity.get('name', 'Unknown')} ({entity.get('type', 'unknown')})")
                        if entity.get('docstring'):
                            doc_preview = entity['docstring'][:80] + "..." if len(entity['docstring']) > 80 else entity['docstring']
                            response.append(f"     {doc_preview}")
                    response.append("")
                
                if overview.get('domains'):
                    response.append("🏢 Business Domains:")
                    for domain in overview['domains']:
                        response.append(f"   • {domain['domain']}: {domain['count']} entities")
                    response.append("")
        
        elif result.intent == QueryIntent.ARCHITECTURAL_ANALYSIS:
            response.append(f"🏗️ Architectural Patterns Found ({len(result.entities)}):")
            for pattern in result.entities:
                response.append(f"   • {pattern['pattern']}: {pattern['entity_count']} entities (avg complexity: {pattern['avg_complexity']})")
        
        elif result.intent in [QueryIntent.FIND_FUNCTION, QueryIntent.FIND_CLASS]:
            response.append(f"📋 Found {len(result.entities)} matching entities:")
            for entity in result.entities[:10]:
                domain_info = f" ({entity.get('business_domain', 'Unknown')})" if entity.get('business_domain') else ""
                team_info = f" [Team: {entity.get('team_ownership')}]" if entity.get('team_ownership') else ""
                response.append(f"   • {entity.get('name', 'Unknown')} ({entity.get('type', 'unknown')}){domain_info}{team_info}")
                response.append(f"     File: {entity.get('file_path', 'unknown')}")
                if entity.get('docstring'):
                    doc_preview = entity['docstring'][:100] + "..." if len(entity['docstring']) > 100 else entity['docstring']
                    response.append(f"     Doc: {doc_preview}")
                response.append("")
        
        elif result.intent == QueryIntent.CODE_QUALITY:
            response.append(f"⚠️ Code Quality Issues Found ({len(result.entities)}):")
            for entity in result.entities[:10]:
                response.append(f"   • {entity.get('name', 'Unknown')} (complexity: {entity.get('complexity', 0)})")
                if entity.get('quality_issues'):
                    for issue in entity['quality_issues']:
                        response.append(f"     - Quality: {issue}")
                if entity.get('style_issues'):
                    for issue in entity['style_issues']:
                        response.append(f"     - Style: {issue}")
                response.append("")
        
        else:
            # Generic entity listing
            response.append(f"📋 Found {len(result.entities)} relevant entities:")
            for entity in result.entities[:10]:
                importance = entity.get('importance_score', entity.get('contextual_score', 1.0))
                response.append(f"   • {entity.get('name', 'Unknown')} ({entity.get('type', 'unknown')}) - Score: {importance:.2f}")
                if entity.get('business_domain'):
                    response.append(f"     Domain: {entity['business_domain']}")
                if entity.get('team_ownership'):
                    response.append(f"     Team: {entity['team_ownership']}")
                response.append("")
        
        # Add context information
        if result.context:
            response.append("📊 Context:")
            if 'domains_found' in result.context and result.context['domains_found']:
                domains = ", ".join(f"{k}({v})" for k, v in result.context['domains_found'].items())
                response.append(f"   Domains: {domains}")
            if 'teams_involved' in result.context and result.context['teams_involved']:
                teams = ", ".join(f"{k}({v})" for k, v in result.context['teams_involved'].items())
                response.append(f"   Teams: {teams}")
            if 'avg_complexity' in result.context:
                response.append(f"   Avg Complexity: {result.context['avg_complexity']}")
        
        # Add suggestions
        if result.suggestions:
            response.append("\n💡 Follow-up suggestions:")
            for suggestion in result.suggestions:
                response.append(f"   • {suggestion}")
        
        # Add metadata
        response.append(f"\n⚡ Query completed in {result.execution_time:.3f}s (confidence: {result.confidence:.2f})")
        
        return "\n".join(response)
    
    def _format_structured(self, result: QueryResult) -> str:
        """Format result in structured format."""
        lines = []
        lines.append(f"Query: {result.query}")
        lines.append(f"Intent: {result.intent}")
        lines.append(f"Results: {len(result.entities)}")
        lines.append(f"Confidence: {result.confidence:.2f}")
        lines.append(f"Execution Time: {result.execution_time:.3f}s")
        lines.append("")
        
        if result.entities:
            lines.append("Entities:")
            for i, entity in enumerate(result.entities[:15], 1):
                lines.append(f"{i:2d}. {entity.get('name', 'Unknown')} ({entity.get('type', 'unknown')})")
                if entity.get('file_path'):
                    lines.append(f"    File: {entity['file_path']}")
                if entity.get('business_domain'):
                    lines.append(f"    Domain: {entity['business_domain']}")
                if entity.get('team_ownership'):
                    lines.append(f"    Team: {entity['team_ownership']}")
        
        return "\n".join(lines)
    
    def get_conversation_context(self) -> Dict[str, Any]:
        """Get context from recent queries for conversation continuity."""
        if not self.query_history:
            return {}
        
        recent_queries = self.query_history[-5:]  # Last 5 queries
        
        # Aggregate context
        all_domains = set()
        all_teams = set()
        common_intents = []
        
        for query in recent_queries:
            common_intents.append(query.intent)
            if query.context:
                domains = query.context.get('domains_found', {})
                all_domains.update(domains.keys())
                teams = query.context.get('teams_involved', {})
                all_teams.update(teams.keys())
        
        return {
            'recent_domains': list(all_domains),
            'recent_teams': list(all_teams),
            'common_intents': common_intents,
            'total_queries': len(self.query_history)
        }

# ===== 4. COMPLETE PIPELINE ORCHESTRATOR =====

class CodebaseSemanticPipeline:
    """
    Complete pipeline that orchestrates the entire semantic indexing process.
    
    This is the main orchestrator class that coordinates:
    1. AST Analysis and Semantic Extraction
    2. Embedding Generation
    3. Knowledge Graph Construction
    4. Search and Query Capabilities
    
    The pipeline is designed to be:
    - Scalable: Uses parallel processing for large codebases
    - Extensible: Easy to add new embedding types or analysis techniques
    - Persistent: Stores results in a durable knowledge graph
    - Queryable: Supports multiple search paradigms
    
    Typical workflow:
    1. Initialize pipeline with output directory
    2. Process codebase to extract and index entities
    3. Use search methods to query the indexed knowledge
    
    Attributes:
        output_dir (Path): Directory for storing index files
        analyzer (SemanticASTAnalyzer): AST analysis component
        embedding_generator (CodeEmbeddingGenerator): Embedding generation component
        knowledge_graph (CodeKnowledgeGraph): Knowledge storage and query component
    """
    
    def __init__(self, output_dir: str = "./code_index", context_config_dir: str = "./context_config"):
        """
        Initialize the complete semantic pipeline with contextual knowledge.
        
        Args:
            output_dir (str): Directory where index files will be stored
            context_config_dir (str): Directory containing contextual configuration files
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.analyzer = SemanticASTAnalyzer(context_config_dir)
        self.embedding_generator = CodeEmbeddingGenerator()
        self.knowledge_graph = CodeKnowledgeGraph(str(self.output_dir / "knowledge.db"))
    
    def process_codebase(self, codebase_path: str):
        """
        Execute the complete pipeline processing workflow.
        
        This method runs the complete semantic indexing pipeline:
        1. Analyzes the codebase structure and extracts entities
        2. Generates embeddings for all discovered entities
        3. Builds and persists the knowledge graph
        4. Generates summary analytics
        
        Args:
            codebase_path (str): Path to the codebase to analyze
            
        Returns:
            Dict: Summary statistics about the processing results
            
        Raises:
            FileNotFoundError: If codebase path doesn't exist
            PermissionError: If output directory cannot be created
        """
        print("🔍 Phase 1: AST Analysis and Semantic Extraction")
        entities, relations = self.analyzer.analyze_codebase(codebase_path)
        print(f"   Extracted {len(entities)} entities, {len(relations)} relations")
        
        print("🧠 Phase 2: Embedding Generation")
        embeddings = self.embedding_generator.generate_embeddings(entities)
        print(f"   Generated embeddings for {len(embeddings)} entities")
        
        print("📊 Phase 3: Knowledge Graph Construction")
        self.knowledge_graph.store_entities_and_embeddings(entities, embeddings)
        self.knowledge_graph.store_relations(relations)
        print("   Knowledge graph constructed and stored")
        
        print("✅ Pipeline completed successfully!")
        
        # Generate summary report
        self._generate_summary_report(entities, relations)
        
        return {
            'entities': len(entities),
            'relations': len(relations),
            'embeddings': len(embeddings),
            'database_path': str(self.output_dir / "knowledge.db")
        }
    
    def search(self, query: str, search_type: str = 'hybrid') -> List[Dict]:
        """
        Search the indexed codebase using various search strategies.
        
        This method provides multiple search approaches:
        - semantic: Vector similarity search using embeddings
        - graph: Graph traversal search for relationships
        - hybrid: Combination of semantic and graph search
        
        Args:
            query (str): Search query (text for semantic, entity_id for graph)
            search_type (str): Type of search ('semantic', 'graph', 'hybrid')
            
        Returns:
            List[Dict]: Search results with relevance scores and metadata
            
        Raises:
            ValueError: If search_type is not supported
        """
        if search_type == 'semantic':
            # Generate query embedding and search
            query_entity = CodeEntity(
                id="query", type="query", name=query, full_name=query,
                source_code=query, docstring=None, file_path="",
                line_start=0, line_end=0, complexity=0,
                dependencies=[], usages=[], context={}
            )
            query_embedding = self.embedding_generator.generate_embeddings([query_entity])
            return self.knowledge_graph.semantic_search(query_embedding["query"])
        
        elif search_type == 'graph':
            # Graph-based search (requires entity_id)
            return self.knowledge_graph.graph_search(query)
        
        else:  # hybrid
            # Combine semantic and graph search
            semantic_results = self.search(query, 'semantic')
            # Enhance with graph information for top results
            enhanced_results = []
            for result in semantic_results[:5]:
                graph_info = self.knowledge_graph.graph_search(result['id'], depth=1)
                result['connections'] = len(graph_info['edges'])
                enhanced_results.append(result)
            
            return enhanced_results
    
    def contextual_search(self, query: str, context_filters: Dict[str, Any] = None) -> List[Dict]:
        """
        Perform contextual search using business domain and team context.
        
        Args:
            query (str): Search query
            context_filters (Dict[str, Any], optional): Context-based filters
            
        Returns:
            List[Dict]: Contextually ranked search results
        """
        return self.knowledge_graph.contextual_search(query, context_filters)
    
    def get_contextual_analytics(self) -> Dict[str, Any]:
        """
        Generate comprehensive contextual analytics about the codebase.
        
        Returns:
            Dict[str, Any]: Analytics including domain and team insights
        """
        analytics = {
            'domains': self.knowledge_graph.get_domain_analytics(),
            'teams': self.knowledge_graph.get_team_analytics(),
            'generated_at': datetime.now().isoformat()
        }
        
        return analytics
    
    def create_ai_agent(self) -> CodebaseAIAgent:
        """
        Create an AI agent interface for querying the knowledge graph.
        
        Returns:
            CodebaseAIAgent: Configured AI agent for natural language queries
        """
        return CodebaseAIAgent(self.knowledge_graph)
    
    def query_agent(self, query: str, format_type: str = "natural") -> str:
        """
        Convenience method to query the AI agent and get formatted response.
        
        Args:
            query (str): Natural language query
            format_type (str): Response format ('natural', 'structured', 'json')
            
        Returns:
            str: Formatted response
        """
        agent = self.create_ai_agent()
        result = agent.query(query)
        return agent.format_response(result, format_type)
    
    def _generate_summary_report(self, entities: List[CodeEntity], relations: List[CodeRelation]):
        """
        Generate comprehensive summary report of the indexed codebase.
        
        This method analyzes the extracted entities and relationships to
        produce insights about the codebase structure, complexity distribution,
        and other key metrics. The report is saved as JSON for later reference.
        
        Args:
            entities (List[CodeEntity]): All extracted entities
            relations (List[CodeRelation]): All discovered relationships
            
        Report includes:
        - Entity type distribution
        - Complexity analysis
        - Relationship type analysis
        - Most complex functions
        - Other structural metrics
        """
        summary = {
            'total_entities': len(entities),
            'entity_types': {},
            'total_relations': len(relations),
            'relation_types': {},
            'complexity_distribution': {'low': 0, 'medium': 0, 'high': 0},
            'top_complex_functions': []
        }
        
        # Analyze entities
        for entity in entities:
            summary['entity_types'][entity.type] = summary['entity_types'].get(entity.type, 0) + 1
            
            complexity_level = entity.context.get('complexity_level', 'medium')
            summary['complexity_distribution'][complexity_level] += 1
            
            if entity.type in ['function', 'method'] and entity.complexity > 5:
                summary['top_complex_functions'].append({
                    'name': entity.full_name,
                    'complexity': entity.complexity,
                    'file': entity.file_path,
                    'business_domain': entity.business_domain,
                    'team_ownership': entity.team_ownership,
                    'importance_score': entity.importance_score
                })
        
        # Add contextual analytics
        summary['business_domains'] = {}
        summary['team_ownership'] = {}
        summary['architectural_patterns'] = {}
        
        for entity in entities:
            if entity.business_domain:
                domain = entity.business_domain
                if domain not in summary['business_domains']:
                    summary['business_domains'][domain] = {'count': 0, 'avg_complexity': 0, 'total_complexity': 0}
                summary['business_domains'][domain]['count'] += 1
                summary['business_domains'][domain]['total_complexity'] += entity.complexity
            
            if entity.team_ownership:
                team = entity.team_ownership
                if team not in summary['team_ownership']:
                    summary['team_ownership'][team] = {'count': 0, 'avg_complexity': 0, 'total_complexity': 0}
                summary['team_ownership'][team]['count'] += 1
                summary['team_ownership'][team]['total_complexity'] += entity.complexity
            
            if entity.architectural_pattern:
                pattern = entity.architectural_pattern
                summary['architectural_patterns'][pattern] = summary['architectural_patterns'].get(pattern, 0) + 1
        
        # Calculate averages
        for domain_data in summary['business_domains'].values():
            if domain_data['count'] > 0:
                domain_data['avg_complexity'] = round(domain_data['total_complexity'] / domain_data['count'], 2)
                del domain_data['total_complexity']
        
        for team_data in summary['team_ownership'].values():
            if team_data['count'] > 0:
                team_data['avg_complexity'] = round(team_data['total_complexity'] / team_data['count'], 2)
                del team_data['total_complexity']
        
        # Analyze relations
        for relation in relations:
            relation_type = relation.relation_type
            summary['relation_types'][relation_type] = summary['relation_types'].get(relation_type, 0) + 1
        
        # Sort complex functions
        summary['top_complex_functions'].sort(key=lambda x: x['complexity'], reverse=True)
        summary['top_complex_functions'] = summary['top_complex_functions'][:10]
        
        # Save summary
        with open(self.output_dir / "summary.json", 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"📈 Summary Report:")
        print(f"   Total Entities: {summary['total_entities']}")
        print(f"   Entity Types: {summary['entity_types']}")
        print(f"   Total Relations: {summary['total_relations']}")
        print(f"   Complexity Distribution: {summary['complexity_distribution']}")
        print(f"   Business Domains: {len(summary['business_domains'])}")
        print(f"   Teams: {len(summary['team_ownership'])}")
        print(f"   Architectural Patterns: {len(summary['architectural_patterns'])}")
        
        # Show domain insights
        if summary['business_domains']:
            print(f"   Top Business Domains:")
            sorted_domains = sorted(summary['business_domains'].items(), 
                                  key=lambda x: x[1]['count'], reverse=True)
            for domain, data in sorted_domains[:3]:
                print(f"     {domain}: {data['count']} entities (avg complexity: {data['avg_complexity']})")
        
        # Show team insights
        if summary['team_ownership']:
            print(f"   Team Distribution:")
            sorted_teams = sorted(summary['team_ownership'].items(), 
                                key=lambda x: x[1]['count'], reverse=True)
            for team, data in sorted_teams[:3]:
                print(f"     {team}: {data['count']} entities (avg complexity: {data['avg_complexity']})")

# ===== COMMAND LINE INTERFACE =====

def parse_arguments():
    """
    Parse and validate command line arguments for the semantic pipeline.
    
    This function sets up a comprehensive argument parser that supports
    multiple operation modes and configuration options:
    
    Operation Modes:
    - --codebase: Index a new codebase
    - --search: Search an existing index
    - --report: Generate report from existing index
    
    Configuration Options:
    - Processing: workers, model, file patterns
    - Search: search type, result limits
    - Output: format, verbosity, file saving
    
    Returns:
        argparse.Namespace: Parsed and validated arguments
        
    Raises:
        SystemExit: If arguments are invalid or conflicting
    """
    parser = argparse.ArgumentParser(
        description="Semantic Code Indexing Pipeline - Extract, embed, and index Python codebases",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Create default configuration files for your project
  python semantic_pipeline.py --create-config --context-config ./my_context

  # Basic usage - index a codebase with contextual knowledge
  python semantic_pipeline.py --codebase /path/to/codebase --output ./index --context-config ./context_config

  # AI Agent natural language queries
  python semantic_pipeline.py --ask "Find all authentication functions" --index ./index
  python semantic_pipeline.py --ask "Show me security-critical code" --index ./index --agent-format structured
  python semantic_pipeline.py --ask "Who owns the payment processing code?" --index ./index
  python semantic_pipeline.py --ask "What are the most complex functions?" --index ./index
  python semantic_pipeline.py --ask "Find functions similar to user_login" --index ./index

  # Traditional search methods
  python semantic_pipeline.py --search "login authentication" --index ./index --search-type contextual --domain Authentication
  python semantic_pipeline.py --search "database" --index ./index --team "Platform Team" --min-importance 1.5

  # Reports and analytics
  python semantic_pipeline.py --report --index ./index --analytics --save-results ./report.json

  # Update existing index (incremental)
  python semantic_pipeline.py --codebase /path/to/codebase --output ./index --incremental
        """
    )
    
    # Main operation modes
    operation_group = parser.add_mutually_exclusive_group(required=True)
    operation_group.add_argument(
        "--codebase", "-c",
        type=str,
        help="Path to the codebase to analyze and index"
    )
    operation_group.add_argument(
        "--search", "-s",
        type=str,
        help="Search query to run against existing index"
    )
    operation_group.add_argument(
        "--report", "-r",
        action="store_true",
        help="Generate report from existing index"
    )
    operation_group.add_argument(
        "--create-config",
        action="store_true",
        help="Create default configuration files"
    )
    operation_group.add_argument(
        "--ask",
        type=str,
        help="Ask the AI agent a natural language question about the codebase"
    )
    
    # Output/Index paths
    parser.add_argument(
        "--output", "-o",
        type=str,
        default="./code_index",
        help="Output directory for index files (default: ./code_index)"
    )
    parser.add_argument(
        "--index", "-i",
        type=str,
        help="Path to existing index (for search/report operations)"
    )
    
    # Processing options
    parser.add_argument(
        "--workers", "-w",
        type=int,
        default=None,
        help="Number of worker processes (default: CPU count)"
    )
    parser.add_argument(
        "--model", "-m",
        type=str,
        default="microsoft/codebert-base",
        help="Model name for embeddings (default: microsoft/codebert-base)"
    )
    parser.add_argument(
        "--file-pattern",
        type=str,
        default="**/*.py",
        help="File pattern to match (default: **/*.py)"
    )
    parser.add_argument(
        "--incremental",
        action="store_true",
        help="Enable incremental processing (only process changed files)"
    )
    
    # Search options
    parser.add_argument(
        "--search-type",
        choices=["semantic", "graph", "hybrid", "contextual"],
        default="hybrid",
        help="Type of search to perform (default: hybrid)"
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=10,
        help="Number of search results to return (default: 10)"
    )
    
    # Contextual search filters
    parser.add_argument(
        "--domain",
        type=str,
        help="Filter by business domain"
    )
    parser.add_argument(
        "--team",
        type=str,
        help="Filter by team ownership"
    )
    parser.add_argument(
        "--pattern",
        type=str,
        help="Filter by architectural pattern"
    )
    parser.add_argument(
        "--min-importance",
        type=float,
        help="Minimum importance score filter"
    )
    parser.add_argument(
        "--security-critical",
        action="store_true",
        help="Show only security-critical entities"
    )
    
    # Output options
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Enable verbose output"
    )
    parser.add_argument(
        "--quiet", "-q",
        action="store_true",
        help="Suppress non-error output"
    )
    parser.add_argument(
        "--output-format",
        choices=["json", "table", "summary"],
        default="summary",
        help="Output format for results (default: summary)"
    )
    parser.add_argument(
        "--save-results",
        type=str,
        help="Save results to file (JSON format)"
    )
    
    # Advanced options
    parser.add_argument(
        "--exclude-tests",
        action="store_true",
        help="Exclude test files from analysis"
    )
    parser.add_argument(
        "--min-complexity",
        type=int,
        default=0,
        help="Minimum complexity threshold for functions (default: 0)"
    )
    parser.add_argument(
        "--max-file-size",
        type=int,
        default=None,
        help="Maximum file size in KB to process"
    )
    parser.add_argument(
        "--analytics",
        action="store_true",
        help="Generate contextual analytics report"
    )
    parser.add_argument(
        "--context-config",
        type=str,
        default="./context_config",
        help="Path to contextual configuration directory (default: ./context_config)"
    )
    parser.add_argument(
        "--agent-format",
        choices=["natural", "structured", "json"],
        default="natural",
        help="Format for AI agent responses (default: natural)"
    )
    
    args = parser.parse_args()
    
    # Validation
    if args.search or args.report:
        if not args.index:
            args.index = args.output
        if not Path(args.index).exists():
            parser.error(f"Index directory '{args.index}' does not exist")
    
    if args.codebase and not Path(args.codebase).exists():
        parser.error(f"Codebase path '{args.codebase}' does not exist")
    
    if args.quiet and args.verbose:
        parser.error("Cannot use --quiet and --verbose together")
    
    return args

def setup_logging(verbose: bool, quiet: bool):
    """
    Configure logging based on verbosity preferences.
    
    Sets up Python logging with appropriate levels and formats:
    - quiet: Only error messages
    - verbose: Debug level with timestamps
    - normal: Info level with simple format
    
    Args:
        verbose (bool): Enable verbose debug output
        quiet (bool): Suppress non-error output
        
    Returns:
        logging.Logger: Configured logger instance
    """
    if quiet:
        logging.basicConfig(level=logging.ERROR, format='%(levelname)s: %(message)s')
    elif verbose:
        logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
    else:
        logging.basicConfig(level=logging.INFO, format='%(message)s')
    
    return logging.getLogger(__name__)

def print_progress(message: str, quiet: bool = False):
    """
    Print timestamped progress message unless in quiet mode.
    
    Args:
        message (str): Progress message to display
        quiet (bool): Whether to suppress output
    """
    if not quiet:
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {message}")

def format_results(results: List[Dict], format_type: str) -> str:
    """
    Format search results in the specified output format.
    
    Supports multiple output formats for different use cases:
    - json: Machine-readable JSON format
    - table: Human-readable tabular format
    - summary: Detailed summary with descriptions
    
    Args:
        results (List[Dict]): Search results to format
        format_type (str): Output format ('json', 'table', 'summary')
        
    Returns:
        str: Formatted results string
        
    Raises:
        ValueError: If format_type is not supported
    """
    if format_type == "json":
        return json.dumps(results, indent=2)
    
    elif format_type == "table":
        if not results:
            return "No results found."
        
        # Simple table formatting with contextual information
        headers = ["Name", "Type", "Score", "Domain", "Team", "File"]
        rows = []
        for result in results:
            score = result.get('contextual_score', result.get('similarity', 0))
            rows.append([
                result.get('name', 'N/A'),
                result.get('type', 'N/A'),
                f"{score:.3f}",
                result.get('business_domain', 'N/A')[:15],
                result.get('team_ownership', 'N/A')[:15],
                result.get('file_path', 'N/A')[-30:]
            ])
        
        # Calculate column widths
        widths = [max(len(str(row[i])) for row in [headers] + rows) for i in range(len(headers))]
        
        # Format table
        table_lines = []
        header_line = " | ".join(h.ljust(w) for h, w in zip(headers, widths))
        table_lines.append(header_line)
        table_lines.append("-" * len(header_line))
        
        for row in rows:
            row_line = " | ".join(str(cell).ljust(w) for cell, w in zip(row, widths))
            table_lines.append(row_line)
        
        return "\n".join(table_lines)
    
    else:  # summary format
        if not results:
            return "No results found."
        
        summary_lines = [f"Found {len(results)} results:\n"]
        for i, result in enumerate(results, 1):
            summary_lines.append(f"{i}. {result.get('name', 'Unknown')}")
            summary_lines.append(f"   Type: {result.get('type', 'N/A')}")
            
            # Show appropriate score
            if 'contextual_score' in result:
                summary_lines.append(f"   Contextual Score: {result['contextual_score']:.3f}")
                summary_lines.append(f"   Importance: {result.get('importance_score', 1.0):.2f}")
            elif 'similarity' in result:
                summary_lines.append(f"   Similarity: {result['similarity']:.3f}")
            
            # Show contextual information
            if 'business_domain' in result and result['business_domain']:
                summary_lines.append(f"   Domain: {result['business_domain']}")
            if 'team_ownership' in result and result['team_ownership']:
                summary_lines.append(f"   Team: {result['team_ownership']}")
            if 'annotations' in result and result['annotations']:
                summary_lines.append(f"   Annotations: {len(result['annotations'])} categories")
            
            if 'connections' in result:
                summary_lines.append(f"   Connections: {result['connections']}")
            
            summary_lines.append("")
        
        return "\n".join(summary_lines)

def main():
    """
    Main entry point for the semantic code indexing pipeline.
    
    This function orchestrates the complete workflow based on command-line
    arguments. It handles three main operation modes:
    
    1. Codebase Processing: Analyzes and indexes a codebase
    2. Search Operations: Queries an existing index
    3. Report Generation: Creates analytics from an index
    
    The function includes comprehensive error handling, progress reporting,
    and performance timing. It ensures graceful handling of interrupts
    and provides detailed error messages for debugging.
    
    Exit Codes:
    - 0: Success
    - 1: General error
    - 130: Keyboard interrupt (Ctrl+C)
    
    Raises:
        SystemExit: On various error conditions or user interrupt
    """
    args = parse_arguments()
    logger = setup_logging(args.verbose, args.quiet)
    
    start_time = time.time()
    
    try:
        if args.codebase:
            # Index/process codebase
            print_progress("🚀 Starting semantic code indexing pipeline", args.quiet)
            print_progress(f"📁 Codebase: {args.codebase}", args.quiet)
            print_progress(f"📊 Output: {args.output}", args.quiet)
            print_progress(f"🔧 Context config: {args.context_config}", args.quiet)
            
            # Initialize pipeline
            pipeline = CodebaseSemanticPipeline(args.output, args.context_config)
            
            if args.workers:
                print_progress(f"⚡ Using {args.workers} worker processes", args.quiet)
            
            # Process the codebase
            results = pipeline.process_codebase(args.codebase)
            
            elapsed = time.time() - start_time
            print_progress(f"✅ Processing completed in {elapsed:.2f} seconds", args.quiet)
            print_progress(f"📈 Results: {results['entities']} entities, {results['relations']} relations", args.quiet)
            
            if args.save_results:
                with open(args.save_results, 'w') as f:
                    json.dump(results, f, indent=2)
                print_progress(f"💾 Results saved to {args.save_results}", args.quiet)
        
        elif args.create_config:
            # Create default configuration files
            print_progress("📁 Creating default configuration files", args.quiet)
            
            # Use the specified context config directory or default
            config_dir = args.context_config if hasattr(args, 'context_config') else "./context_config"
            
            # Create a temporary loader to access the default configs
            temp_loader = ContextualKnowledgeLoader("/nonexistent")  # This will trigger default loading
            temp_loader.create_default_config_files(config_dir)
            
            print_progress("✅ Configuration files created successfully!", args.quiet)
            print_progress(f"💡 Edit the files in '{config_dir}' to customize for your project", args.quiet)
        
        elif args.ask:
            # AI Agent Query
            print_progress(f"🤖 Asking AI agent: '{args.ask}'", args.quiet)
            
            # Initialize pipeline with existing index
            index_path = args.index if args.index else args.output
            if not Path(index_path).exists():
                print("❌ Index not found. Please run indexing first or specify correct --index path", file=sys.stderr)
                sys.exit(1)
            
            pipeline = CodebaseSemanticPipeline(index_path, args.context_config)
            
            # Query the AI agent
            response = pipeline.query_agent(args.ask, args.agent_format)
            
            if not args.quiet:
                print(f"\n🤖 AI Agent Response:")
                print("=" * 50)
            print(response)
            
            if args.save_results:
                # Save the raw result as JSON
                agent = pipeline.create_ai_agent()
                result = agent.query(args.ask)
                with open(args.save_results, 'w') as f:
                    json.dump(asdict(result), f, indent=2, default=str)
                print_progress(f"💾 Agent result saved to {args.save_results}", args.quiet)
        
        elif args.search:
            # Search existing index
            print_progress(f"🔍 Searching index: {args.index}", args.quiet)
            print_progress(f"🎯 Query: '{args.search}'", args.quiet)
            print_progress(f"📋 Search type: {args.search_type}", args.quiet)
            
            # Initialize pipeline with existing index
            pipeline = CodebaseSemanticPipeline(args.index, args.context_config)
            
            # Prepare context filters for contextual search
            context_filters = {}
            if args.domain:
                context_filters['business_domain'] = args.domain
            if args.team:
                context_filters['team_ownership'] = args.team
            if args.pattern:
                context_filters['architectural_pattern'] = args.pattern
            if args.min_importance:
                context_filters['min_importance'] = args.min_importance
            if args.security_critical:
                context_filters['has_security_annotations'] = True
            
            # Perform search
            if args.search_type == 'contextual':
                search_results = pipeline.contextual_search(args.search, context_filters)
            else:
                search_results = pipeline.search(args.search, search_type=args.search_type)
            
            # Limit results
            search_results = search_results[:args.top_k]
            
            # Format and display results
            formatted_results = format_results(search_results, args.output_format)
            
            if not args.quiet:
                print(f"\n🔍 Search Results ({len(search_results)} found):")
                print("=" * 50)
            print(formatted_results)
            
            if args.save_results:
                with open(args.save_results, 'w') as f:
                    json.dump(search_results, f, indent=2)
                print_progress(f"💾 Search results saved to {args.save_results}", args.quiet)
        
        elif args.report:
            # Generate report from existing index
            print_progress(f"📊 Generating report from index: {args.index}", args.quiet)
            
            # Initialize pipeline
            pipeline = CodebaseSemanticPipeline(args.index, args.context_config)
            
            # Load summary file
            summary_path = Path(args.index) / "summary.json"
            if summary_path.exists():
                with open(summary_path, 'r') as f:
                    summary = json.load(f)
                
                if not args.quiet:
                    print("\n📈 Codebase Analysis Report")
                    print("=" * 50)
                
                print(f"Total Entities: {summary['total_entities']:,}")
                print(f"Total Relations: {summary['total_relations']:,}")
                print(f"\nEntity Distribution:")
                for entity_type, count in summary['entity_types'].items():
                    print(f"  {entity_type}: {count:,}")
                
                print(f"\nComplexity Distribution:")
                for level, count in summary['complexity_distribution'].items():
                    print(f"  {level}: {count:,}")
                
                # Show contextual information
                if 'business_domains' in summary and summary['business_domains']:
                    print(f"\nBusiness Domains:")
                    for domain, data in summary['business_domains'].items():
                        print(f"  {domain}: {data['count']} entities (avg complexity: {data['avg_complexity']})")
                
                if 'team_ownership' in summary and summary['team_ownership']:
                    print(f"\nTeam Ownership:")
                    for team, data in summary['team_ownership'].items():
                        print(f"  {team}: {data['count']} entities (avg complexity: {data['avg_complexity']})")
                
                if 'architectural_patterns' in summary and summary['architectural_patterns']:
                    print(f"\nArchitectural Patterns:")
                    for pattern, count in summary['architectural_patterns'].items():
                        print(f"  {pattern}: {count} entities")
                
                if summary['top_complex_functions']:
                    print(f"\nMost Complex Functions:")
                    for func in summary['top_complex_functions'][:5]:
                        domain_info = f" ({func.get('business_domain', 'N/A')})" if func.get('business_domain') else ""
                        print(f"  {func['name']}: {func['complexity']}{domain_info}")
                
                # Generate contextual analytics if requested
                if args.analytics:
                    print_progress(f"🧠 Generating contextual analytics...", args.quiet)
                    analytics = pipeline.get_contextual_analytics()
                    
                    print(f"\n🧠 Contextual Analytics:")
                    print("=" * 30)
                    
                    if analytics['domains']:
                        print(f"\nDomain Analytics:")
                        for domain, data in analytics['domains'].items():
                            print(f"  {domain}:")
                            print(f"    Entities: {data['entity_count']}")
                            print(f"    Avg Complexity: {data['avg_complexity']}")
                            print(f"    Avg Importance: {data['avg_importance']}")
                            print(f"    Annotated: {data['annotated_entities']}")
                    
                    if analytics['teams']:
                        print(f"\nTeam Analytics:")
                        for team, data in analytics['teams'].items():
                            print(f"  {team}:")
                            print(f"    Entities: {data['entity_count']}")
                            print(f"    Quality Score: {data['quality_score']}%")
                            print(f"    Quality Issues: {data['quality_issues_count']}")
                            print(f"    Style Issues: {data['style_issues_count']}")
                
                if args.save_results:
                    report_data = summary
                    if args.analytics:
                        report_data['contextual_analytics'] = analytics
                    
                    with open(args.save_results, 'w') as f:
                        json.dump(report_data, f, indent=2)
                    print_progress(f"💾 Report saved to {args.save_results}", args.quiet)
            else:
                print("❌ No summary report found in index directory", file=sys.stderr)
                sys.exit(1)
    
    except KeyboardInterrupt:
        print_progress("⏹️ Process interrupted by user", args.quiet)
        sys.exit(130)
    
    except Exception as e:
        logger.error(f"❌ Error: {str(e)}")
        if args.verbose:
            traceback.print_exc()
        sys.exit(1)
    
    finally:
        elapsed = time.time() - start_time
        print_progress(f"⏱️ Total runtime: {elapsed:.2f} seconds", args.quiet)

# ===== USAGE EXAMPLES =====
if __name__ == "__main__":
    main()
